{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaykumaran/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jaykumaran/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Florence-2-large-ft:\n",
      "- configuration_florence2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Florence-2-large-ft:\n",
      "- modeling_florence2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/home/jaykumaran/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Florence-2-large-ft:\n",
      "- processing_florence2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from unittest.mock import patch\n",
    "import spaces\n",
    "import gradio as gr\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from transformers.dynamic_module_utils import get_imports\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import io\n",
    "import uuid\n",
    "\n",
    "def workaround_fixed_get_imports(filename: str | os.PathLike) -> list[str]:\n",
    "    if not str(filename).endswith(\"/modeling_florence2.py\"):\n",
    "        return get_imports(filename)\n",
    "    imports = get_imports(filename)\n",
    "    imports.remove(\"flash_attn\")\n",
    "    return imports\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with patch(\"transformers.dynamic_module_utils.get_imports\", workaround_fixed_get_imports):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large-ft\", trust_remote_code=True).to(device).eval()\n",
    "    processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large-ft\", trust_remote_code=True)\n",
    "\n",
    "colormap = ['blue', 'orange', 'green', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan', 'red',\n",
    "            'lime', 'indigo', 'violet', 'aqua', 'magenta', 'coral', 'gold', 'tan', 'skyblue']\n",
    "\n",
    "def run_example(task_prompt, image, text_input=None):\n",
    "    prompt = task_prompt if text_input is None else task_prompt + text_input\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=1024, early_stopping=False, do_sample=False, num_beams=3)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    return processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.size[0], image.size[1]))\n",
    "\n",
    "def fig_to_pil(fig):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png', dpi=300, bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    return Image.open(buf)\n",
    "\n",
    "def plot_bbox_img(image, data):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    if 'bboxes' in data and 'labels' in data:\n",
    "        bboxes, labels = data['bboxes'], data['labels']\n",
    "    elif 'bboxes' in data and 'bboxes_labels' in data:\n",
    "        bboxes, labels = data['bboxes'], data['bboxes_labels']\n",
    "    else:\n",
    "        return fig_to_pil(fig)\n",
    "\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='indigo', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(x1, y1, label, color='white', fontsize=10, bbox=dict(facecolor='indigo', alpha=0.8))\n",
    "    \n",
    "    ax.axis('off')\n",
    "    return fig_to_pil(fig)\n",
    "\n",
    "def draw_poly_img(image, prediction, fill_mask=False):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(image)\n",
    "    for polygons, label in zip(prediction.get('polygons', []), prediction.get('labels', [])):\n",
    "        color = random.choice(colormap)\n",
    "        for polygon in polygons:\n",
    "            if isinstance(polygon[0], (int, float)):\n",
    "                polygon = [(polygon[i], polygon[i+1]) for i in range(0, len(polygon), 2)]\n",
    "            poly = patches.Polygon(polygon, edgecolor=color, facecolor=color if fill_mask else 'none', alpha=0.5 if fill_mask else 1, linewidth=2)\n",
    "            ax.add_patch(poly)\n",
    "        if polygon:\n",
    "            plt.text(polygon[0][0], polygon[0][1], label, color='white', fontsize=10, bbox=dict(facecolor=color, alpha=0.8))\n",
    "    ax.axis('off')\n",
    "    return fig_to_pil(fig)\n",
    "\n",
    "def draw_ocr_bboxes(image, prediction):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(image)\n",
    "    bboxes, labels = prediction['quad_boxes'], prediction['labels']\n",
    "    for box, label in zip(bboxes, labels):\n",
    "        color = random.choice(colormap)\n",
    "        box_array = np.array(box).reshape(-1, 2)  # respect format\n",
    "        polygon = patches.Polygon(box_array, edgecolor=color, fill=False, linewidth=2)\n",
    "        ax.add_patch(polygon)\n",
    "        plt.text(box_array[0, 0], box_array[0, 1], label, color='white', fontsize=10, bbox=dict(facecolor=color, alpha=0.8))\n",
    "    ax.axis('off')\n",
    "    return fig_to_pil(fig)\n",
    "\n",
    "def plot_bbox(image, data):\n",
    "    img_draw = image.copy()\n",
    "    draw = ImageDraw.Draw(img_draw)\n",
    "    for bbox, label in zip(data['bboxes'], data['labels']):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "        draw.text((x1, y1), label, fill=\"white\")\n",
    "    return np.array(img_draw)\n",
    "\n",
    "@spaces.GPU(duration=130) #remains to be seen, increasing too much may leave people queueing for long\n",
    "def process_video(input_video_path, task_prompt):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        return None, []\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    result_file_name = f\"{uuid.uuid4()}.mp4\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(result_file_name, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    processed_frames = 0\n",
    "    frame_results = []\n",
    "    color_map = {}  #consistency for chromakey possibility\n",
    "\n",
    "    def get_color(label):\n",
    "        if label not in color_map:\n",
    "            color_map[label] = random.choice(colormap)\n",
    "        return color_map[label]\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "\n",
    "        try:\n",
    "            result = run_example(task_prompt, pil_image)\n",
    "\n",
    "            if task_prompt == \"<OD>\":\n",
    "                processed_image = plot_bbox(pil_image, result['<OD>'])\n",
    "                frame_results.append((processed_frames + 1, result['<OD>']))\n",
    "            elif task_prompt == \"<DENSE_REGION_CAPTION>\":\n",
    "                processed_image = pil_image.copy()\n",
    "                draw = ImageDraw.Draw(processed_image)\n",
    "                for i, label in enumerate(result['<DENSE_REGION_CAPTION>'].get('labels', [])):\n",
    "                    draw.text((10, 10 + i*20), label, fill=\"white\")\n",
    "                processed_image = np.array(processed_image)\n",
    "                frame_results.append((processed_frames + 1, result['<DENSE_REGION_CAPTION>']))\n",
    "            elif task_prompt in [\"<REFERRING_EXPRESSION_SEGMENTATION>\", \"<REGION_TO_SEGMENTATION>\"]:\n",
    "                if isinstance(result[task_prompt], dict) and 'polygons' in result[task_prompt]:\n",
    "                    processed_image = draw_vid_polygons(pil_image, result[task_prompt], get_color)\n",
    "                else:\n",
    "                    processed_image = np.array(pil_image)\n",
    "                frame_results.append((processed_frames + 1, result[task_prompt]))\n",
    "            else:\n",
    "                processed_image = np.array(pil_image)\n",
    "\n",
    "            out.write(cv2.cvtColor(processed_image, cv2.COLOR_RGB2BGR))\n",
    "            processed_frames += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {processed_frames + 1}: {str(e)}\")\n",
    "            processed_image = np.array(pil_image)\n",
    "            out.write(cv2.cvtColor(processed_image, cv2.COLOR_RGB2BGR))\n",
    "            processed_frames += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if processed_frames == 0:\n",
    "        return None, frame_results\n",
    "\n",
    "    return result_file_name, frame_results\n",
    "\n",
    "def draw_vid_polygons(image, prediction, get_color):\n",
    "    img_draw = image.copy()\n",
    "    draw = ImageDraw.Draw(img_draw)\n",
    "    for polygons, label in zip(prediction.get('polygons', []), prediction.get('labels', [])):\n",
    "        color = get_color(label)\n",
    "        for polygon in polygons:\n",
    "            if isinstance(polygon[0], (int, float)): \n",
    "                polygon = [(polygon[i], polygon[i+1]) for i in range(0, len(polygon), 2)]\n",
    "            draw.polygon(polygon, outline=color, fill=color)\n",
    "        if polygon:\n",
    "            draw.text(polygon[0], label, fill=\"white\")\n",
    "    return np.array(img_draw)\n",
    "\n",
    "def process_image(image, task, text):\n",
    "    task_mapping = {\n",
    "        \"Caption\": (\"<CAPTION>\", lambda result: (result['<CAPTION>'], image)),\n",
    "        \"Detailed Caption\": (\"<DETAILED_CAPTION>\", lambda result: (result['<DETAILED_CAPTION>'], image)),\n",
    "        \"More Detailed Caption\": (\"<MORE_DETAILED_CAPTION>\", lambda result: (result['<MORE_DETAILED_CAPTION>'], image)),\n",
    "        \"Caption to Phrase Grounding\": (\"<CAPTION_TO_PHRASE_GROUNDING>\", lambda result: (str(result['<CAPTION_TO_PHRASE_GROUNDING>']), plot_bbox_img(image, result['<CAPTION_TO_PHRASE_GROUNDING>']))),\n",
    "        \"Object Detection\": (\"<OD>\", lambda result: (str(result['<OD>']), plot_bbox_img(image, result['<OD>']))),\n",
    "        \"Dense Region Caption\": (\"<DENSE_REGION_CAPTION>\", lambda result: (str(result['<DENSE_REGION_CAPTION>']), plot_bbox_img(image, result['<DENSE_REGION_CAPTION>']))),\n",
    "        \"Region Proposal\": (\"<REGION_PROPOSAL>\", lambda result: (str(result['<REGION_PROPOSAL>']), plot_bbox_img(image, result['<REGION_PROPOSAL>']))),\n",
    "        \"Referring Expression Segmentation\": (\"<REFERRING_EXPRESSION_SEGMENTATION>\", lambda result: (str(result['<REFERRING_EXPRESSION_SEGMENTATION>']), draw_poly_img(image, result['<REFERRING_EXPRESSION_SEGMENTATION>'], fill_mask=True))),\n",
    "        \"Region to Segmentation\": (\"<REGION_TO_SEGMENTATION>\", lambda result: (str(result['<REGION_TO_SEGMENTATION>']), draw_poly_img(image, result['<REGION_TO_SEGMENTATION>'], fill_mask=True))),\n",
    "        \"Open Vocabulary Detection\": (\"<OPEN_VOCABULARY_DETECTION>\", lambda result: (str(result['<OPEN_VOCABULARY_DETECTION>']), plot_bbox_img(image, result['<OPEN_VOCABULARY_DETECTION>']))),\n",
    "        \"Region to Category\": (\"<REGION_TO_CATEGORY>\", lambda result: (result['<REGION_TO_CATEGORY>'], image)),\n",
    "        \"Region to Description\": (\"<REGION_TO_DESCRIPTION>\", lambda result: (result['<REGION_TO_DESCRIPTION>'], image)),\n",
    "        \"OCR\": (\"<OCR>\", lambda result: (result['<OCR>'], image)),\n",
    "        \"OCR with Region\": (\"<OCR_WITH_REGION>\", lambda result: (str(result['<OCR_WITH_REGION>']), draw_ocr_bboxes(image, result['<OCR_WITH_REGION>']))),\n",
    "    }\n",
    "\n",
    "    if task in task_mapping:\n",
    "        prompt, process_func = task_mapping[task]\n",
    "        result = run_example(prompt, image, text)\n",
    "        return process_func(result)\n",
    "    else:\n",
    "        return \"\", image\n",
    "\n",
    "def map_task_to_prompt(task):\n",
    "    task_mapping = {\n",
    "        \"Object Detection\": \"<OD>\",\n",
    "        \"Dense Region Caption\": \"<DENSE_REGION_CAPTION>\",\n",
    "        \"Referring Expression Segmentation\": \"<REFERRING_EXPRESSION_SEGMENTATION>\",\n",
    "        \"Region to Segmentation\": \"<REGION_TO_SEGMENTATION>\"\n",
    "    }\n",
    "    return task_mapping.get(task, \"\")\n",
    "\n",
    "def process_video_p(input_video, task, text_input):\n",
    "    prompt = map_task_to_prompt(task)\n",
    "    if task == \"Referring Expression Segmentation\" and text_input:\n",
    "        prompt += text_input\n",
    "    result, frame_results = process_video(input_video, prompt)\n",
    "    if result is None:\n",
    "        return None, \"Error: Video processing failed. Check logs above for info.\", str(frame_results)\n",
    "    return result, result, str(frame_results)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"<h1><center>Microsoft Florence-2-large-ft</center></h1>\")\n",
    "    \n",
    "    with gr.Tab(label=\"Image\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                input_img = gr.Image(label=\"Input Picture\", type=\"pil\")\n",
    "                task_dropdown = gr.Dropdown(\n",
    "                    choices=[\"Caption\", \"Detailed Caption\", \"More Detailed Caption\", \"Caption to Phrase Grounding\",\n",
    "                             \"Object Detection\", \"Dense Region Caption\", \"Region Proposal\", \"Referring Expression Segmentation\",\n",
    "                             \"Region to Segmentation\", \"Open Vocabulary Detection\", \"Region to Category\", \"Region to Description\",\n",
    "                             \"OCR\", \"OCR with Region\"],\n",
    "                    label=\"Task\", value=\"Caption\"\n",
    "                )\n",
    "                text_input = gr.Textbox(label=\"Text Input (is Optional)\", visible=False)\n",
    "                gr.Examples(\n",
    "                    examples=[\n",
    "                        [\n",
    "                            \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\",\n",
    "                            \"Detailed Caption\",\n",
    "                            \"\",\n",
    "                        ],\n",
    "                        [\n",
    "                            \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\",\n",
    "                            \"Object Detection\",\n",
    "                            \"\",\n",
    "                        ],\n",
    "                        [\n",
    "                            \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\",\n",
    "                            \"Caption to Phrase Grounding\",\n",
    "                            \"A green car parked in front of a yellow building.\"\n",
    "                        ],\n",
    "                        [\n",
    "                            \"http://ecx.images-amazon.com/images/I/51UUzBDAMsL.jpg?download=true\",\n",
    "                            \"OCR\",\n",
    "                            \"\"\n",
    "                        ]\n",
    "                    ],\n",
    "                    inputs=[input_img, task_dropdown, text_input],\n",
    "                )\n",
    "                submit_btn = gr.Button(value=\"Submit\")\n",
    "            with gr.Column():\n",
    "                output_text = gr.Textbox(label=\"Results\")\n",
    "                output_image = gr.Image(label=\"Image\", type=\"pil\")\n",
    "\n",
    "    with gr.Tab(label=\"Video\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                input_video = gr.Video(label=\"Video\")\n",
    "                video_task_dropdown = gr.Dropdown(\n",
    "                    choices=[\"Object Detection\", \"Dense Region Caption\", \"Referring Expression Segmentation\", \"Region to Segmentation\"],\n",
    "                    label=\"Video Task\", value=\"Object Detection\"\n",
    "                )\n",
    "                video_text_input = gr.Textbox(label=\"Text Input (for Referring Expression Segmentation)\", visible=False)\n",
    "                video_submit_btn = gr.Button(value=\"Process Video\")\n",
    "            with gr.Column():\n",
    "                output_video = gr.Video(label=\"Processed Video\")\n",
    "                frame_results_output = gr.Textbox(label=\"Frame Results\")\n",
    "\n",
    "    def update_text_input(task):\n",
    "        return gr.update(visible=task in [\"Caption to Phrase Grounding\", \"Referring Expression Segmentation\",\n",
    "                                           \"Region to Segmentation\", \"Open Vocabulary Detection\", \"Region to Category\",\n",
    "                                           \"Region to Description\"])\n",
    "\n",
    "    task_dropdown.change(fn=update_text_input, inputs=task_dropdown, outputs=text_input)\n",
    "\n",
    "    def update_video_text_input(task):\n",
    "        return gr.update(visible=task == \"Referring Expression Segmentation\")\n",
    "\n",
    "    video_task_dropdown.change(fn=update_video_text_input, inputs=video_task_dropdown, outputs=video_text_input)\n",
    "\n",
    "    submit_btn.click(fn=process_image, inputs=[input_img, task_dropdown, text_input], outputs=[output_text, output_image])\n",
    "    video_submit_btn.click(fn=process_video_p, inputs=[input_video, video_task_dropdown, video_text_input], outputs=[output_video, output_video, frame_results_output])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img scr=\"https://learnopencv.com/wp-content/uploads/2024/07/Florence-2-Inference-Captioning-Task-CVPR2024-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://learnopencv.com/wp-content/uploads/2024/07/Florence-2-Inference-Captioning-Task-CVPR2024-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://learnopencv.com/wp-content/uploads/2024/07/Florence-2-Inference-Open-Vocab-Detection-Task-CVPR2024-1-1024x541.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://learnopencv.com/wp-content/uploads/2024/07/Florence-2-Inference-OCR-Task-CVPR2024-2.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HuggingFace",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
