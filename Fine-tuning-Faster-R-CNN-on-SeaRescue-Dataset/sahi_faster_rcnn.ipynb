{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysV8O20fyWJL"
   },
   "source": [
    "# SAHI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hx-4F2dByWJM"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "  <img src=\"https://learnopencv.com/wp-content/uploads/2023/06/sliced_inference.gif\" alt=\"Image\"  width=500 height=400>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDLD4I38yWJN"
   },
   "source": [
    "\n",
    "Traditional object detection models often struggle with small objects due to their limited size and the contextual information available in an image. That’s where SAHI comes into play to charm with its great results. SAHI addresses this by employing techniques specifically focusing on augmenting the dataset to highlight these small instances. It enhances the training process by using methods like slicing images into smaller patches where small objects become more prominent and easier to detect.\n",
    "\n",
    "To learn more about Sliced Aided Hyper Inference (SAHI), bookmark this for later.\n",
    "\n",
    "Link: https://learnopencv.com/slicing-aided-hyper-inference/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "of9EPjeeyWJN"
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf5aCIlxyWJO",
    "outputId": "613c23d5-831e-4623-e6a3-f8449209c276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq -U sahi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "m-Xd5ObjyWJP"
   },
   "outputs": [],
   "source": [
    "# import required functions, classes\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageOps, ImageStat\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Patch\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction, predict, get_prediction\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.prediction import visualize_object_predictions\n",
    "from sahi.utils.cv import read_image\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import v2 as Tv2\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torchvision.models.detection as detection\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RbtwA9RyWJP"
   },
   "source": [
    "Let's set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FfynSZR8yWJP"
   },
   "outputs": [],
   "source": [
    "def set_seeds():\n",
    "    # fix random seeds\n",
    "    SEED_VALUE = 42\n",
    "\n",
    "    random.seed(SEED_VALUE)\n",
    "    np.random.seed(SEED_VALUE)\n",
    "    torch.manual_seed(SEED_VALUE)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED_VALUE)\n",
    "        torch.cuda.manual_seed_all(SEED_VALUE)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5mt7_s_r1vlz"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/SeaDroneSee_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yye-TSpoyWJP"
   },
   "source": [
    "## Download test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tIAQFmgwyWJP"
   },
   "outputs": [],
   "source": [
    "# Ensure the base directory exists\n",
    "if not os.path.exists('SeaDroneSee_test'):\n",
    "    os.mkdir('SeaDroneSee_test')\n",
    "\n",
    "# Ensure the Model_ckpt directory exists inside the base directory\n",
    "if not os.path.exists('SeaDroneSee_test/Model_ckpt'):\n",
    "    os.mkdir('SeaDroneSee_test/Model_ckpt')\n",
    "\n",
    "def download_file(url, save_name):\n",
    "    if not os.path.exists(save_name):\n",
    "        # Handling potential redirection in requests\n",
    "        with requests.get(url, allow_redirects=True) as r:\n",
    "            if r.status_code == 200:\n",
    "                with open(save_name, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "            else:\n",
    "                print(\"Failed to download the file, status code:\", r.status_code)\n",
    "\n",
    "def unzip(zip_file=None, target_dir=None):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file, 'r') as z:\n",
    "            z.extractall(target_dir)\n",
    "            print(\"Extracted all to:\", target_dir)\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Invalid file or error during extraction: Bad Zip File\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "# Correct Dropbox link for test images (Ensure this is the direct download link or properly redirects)\n",
    "download_url = 'https://www.dropbox.com/scl/fi/4qidpahgu9mogam33uxlz/SeaDroneSee_test.zip?rlkey=1gt6mebuppxg4ehzhicwqafav&st=rtuwbmuo&dl=1'\n",
    "save_path = 'SeaDroneSee_test/SeaDroneSee_test.zip'\n",
    "download_file(download_url, save_path)\n",
    "\n",
    "# Correct Dropbox link for model checkpoint\n",
    "model_ckpt_url = 'https://www.dropbox.com/scl/fi/xmftrum0a8rgjp82j6n65/model_ckpt.zip?rlkey=aywwl28rbcbiejggdps0durfu&st=dda61bld&dl=1'\n",
    "model_save_path = 'SeaDroneSee_test/Model_ckpt.zip'\n",
    "download_file(model_ckpt_url, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TGZWlvL_ziRk",
    "outputId": "ee3e8e74-cd5a-435c-a536-abfd8f912f68"
   },
   "outputs": [],
   "source": [
    "# Unzip test images to SeaDroneSee_test\n",
    "unzip(zip_file=save_path, target_dir='SeaDroneSee_test')\n",
    "# Unzip model checkpoint to Model_ckpt folder inside SeaDroneSee_test\n",
    "unzip(zip_file=model_save_path, target_dir='SeaDroneSee_test/Model_ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 38
    },
    "id": "msmEkDUa8obg",
    "outputId": "7f75b017-6537-43da-fac8-a64cccbabce7"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9F4ZtNiDyWJP",
    "outputId": "12653a66-6a20-43b7-accb-0261ddaaee4e"
   },
   "outputs": [],
   "source": [
    "cd SeaDroneSee_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtv3DMU5yWJQ"
   },
   "source": [
    "#### Class Mapping\n",
    "\n",
    "Let's do a class mapping and assign a unique color for each label or class ID,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UF-Q2wxFyWJQ"
   },
   "outputs": [],
   "source": [
    "classes_to_idx = {\n",
    "    0: 'ignored',\n",
    "    1: 'swimmer',\n",
    "    2: 'boat',\n",
    "    3: 'jetski',\n",
    "    4: 'life_saving_appliances',\n",
    "    5: \"buoy\"\n",
    "}\n",
    "\n",
    "# Mapping category IDs to colors\n",
    "category_colors = {\n",
    "    0: 'black',   # ignored\n",
    "    1: 'red',     # swimmer\n",
    "    2: 'orange',   # boat\n",
    "    3: 'blue',    # jetski\n",
    "    4: 'purple',  # life saving appliances\n",
    "    5: 'yellow'   # buoy\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiEC4bAFyWJQ"
   },
   "source": [
    "## Load Fine-tuned Faster RCNN Model Checkpoint\n",
    "\n",
    "Let’s load the best model checkpoint and adjust the box_nms_thresh of 0.3 as a postprocessing step to avoid overlapping bounding boxes from same class instances. As we will use our fine-tuned FasterRCNN model, the pretrained=False argument is passed. The model’s state dictionary passes checks across all the layers, and for inference the model is set to eval mode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UPlUEqoryWJQ",
    "outputId": "6bf9de4a-c730-4d91-f779-c68f54e3c3bf"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"Model_ckpt/model_ckpt/Mobilenet_5e-4_best_model_checkpoint_epoch_27.pth\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Function to load the trained model\n",
    "def load_model(checkpoint_path, device):\n",
    "    model = detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=False, num_classes=len(classes_to_idx),box_nms_thresh=0.3)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "model = load_model(checkpoint_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FiU1wJXHyWJQ"
   },
   "outputs": [],
   "source": [
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "   model_type='torchvision',\n",
    "   model=model, #Faster RCNN Model\n",
    "   confidence_threshold=0.7,\n",
    "   image_size=5436, #Image's longest dimension\n",
    "   device=\"cpu\", # or \"cuda:0\"\n",
    "   load_at_init=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSbFRc5YyWJQ"
   },
   "source": [
    "Using slice height and slice width we can control the dimension of the sliding window. As our model trained on a patch dimension of half the size of image dimensions we will choose the slice width and slice height accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2xipcd3yWJQ",
    "outputId": "3d482fc3-b80b-4b3d-cf78-8f39ea9fd5b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7882\n"
     ]
    }
   ],
   "source": [
    "img_path = 'test/7882.jpg'\n",
    "img_filename_temp = img_path.split('/')[1]\n",
    "img_filename = img_filename_temp.split('.')[0]\n",
    "\n",
    "# print(img_filename)\n",
    "img_pil = PIL.Image.open(img_path)\n",
    "W,H = img_pil.size\n",
    "# print(W)\n",
    "s_h,s_w = H/2,W/2\n",
    "s_h ,s_w = int(s_h),int(s_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qBlwZbuyWJR"
   },
   "source": [
    "\n",
    "The get_sliced_prediction returns a list of detected object instances with their bbox, score and category id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26PngQSoyWJR",
    "outputId": "68ae9a0f-d4dc-4ab7-948f-11dbe70c16c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing prediction on 9 slices.\n"
     ]
    }
   ],
   "source": [
    "result = get_sliced_prediction(\n",
    "   img_path,\n",
    "   detection_model,\n",
    "   slice_height=s_h,\n",
    "   slice_width=s_w,\n",
    "   overlap_height_ratio=0.2,\n",
    "   overlap_width_ratio=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWZUPvrKyWJR"
   },
   "source": [
    "Here we can see the class id is correct but corresponding label id is in accordance to COCO classes. So we will fix this by defining some custom functions that does class mapping and draw the bbox which matches the category id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SW8P3OHAyWJR",
    "outputId": "535c6ef8-8525-43bc-a02c-6fcca040ef05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObjectPrediction<\n",
       "     bbox: BoundingBox: <(1825.7972106933594, 876.7810821533203, 1899.0037841796875, 908.668701171875), w: 73.20657348632812, h: 31.887619018554688>,\n",
       "     mask: None,\n",
       "     score: PredictionScore: <value: 0.9916843175888062>,\n",
       "     category: Category: <id: 1, name: person>>,\n",
       " ObjectPrediction<\n",
       "     bbox: BoundingBox: <(1271.9078369140625, 1302.0615844726562, 1319.986328125, 1335.0255432128906), w: 48.0784912109375, h: 32.963958740234375>,\n",
       "     mask: None,\n",
       "     score: PredictionScore: <value: 0.9903111457824707>,\n",
       "     category: Category: <id: 1, name: person>>,\n",
       " ObjectPrediction<\n",
       "     bbox: BoundingBox: <(1842.816162109375, 1008.3194732666016, 1934.3472290039062, 1044.4426727294922), w: 91.53106689453125, h: 36.123199462890625>,\n",
       "     mask: None,\n",
       "     score: PredictionScore: <value: 0.9834246039390564>,\n",
       "     category: Category: <id: 1, name: person>>,\n",
       " ObjectPrediction<\n",
       "     bbox: BoundingBox: <(2409.4612426757812, 1173.730094909668, 2436.6535034179688, 1199.1522216796875), w: 27.1922607421875, h: 25.42212677001953>,\n",
       "     mask: None,\n",
       "     score: PredictionScore: <value: 0.9819782376289368>,\n",
       "     category: Category: <id: 1, name: person>>,\n",
       " ObjectPrediction<\n",
       "     bbox: BoundingBox: <(2367.310791015625, 1095.7762565612793, 2408.7229614257812, 1125.3503723144531), w: 41.41217041015625, h: 29.574115753173828>,\n",
       "     mask: None,\n",
       "     score: PredictionScore: <value: 0.9754398465156555>,\n",
       "     category: Category: <id: 1, name: person>>,\n",
       " ObjectPrediction<\n",
       "     bbox: BoundingBox: <(1660.670654296875, 905.3052978515625, 1698.0181884765625, 945.0361938476562), w: 37.3475341796875, h: 39.73089599609375>,\n",
       "     mask: None,\n",
       "     score: PredictionScore: <value: 0.9750140905380249>,\n",
       "     category: Category: <id: 1, name: person>>,\n",
       " ObjectPrediction<\n",
       "     bbox: BoundingBox: <(1352.75634765625, 1161.37353515625, 1380.359375, 1187.2816162109375), w: 27.60302734375, h: 25.9080810546875>,\n",
       "     mask: None,\n",
       "     score: PredictionScore: <value: 0.9734489917755127>,\n",
       "     category: Category: <id: 1, name: person>>,\n",
       " ObjectPrediction<\n",
       "     bbox: BoundingBox: <(2522.926025390625, 1103.2628059387207, 2551.4114379882812, 1132.750717163086), w: 28.48541259765625, h: 29.487911224365234>,\n",
       "     mask: None,\n",
       "     score: PredictionScore: <value: 0.9666545391082764>,\n",
       "     category: Category: <id: 1, name: person>>,\n",
       " ObjectPrediction<\n",
       "     bbox: BoundingBox: <(2373.580322265625, 1206.2977294921875, 2433.8546752929688, 1234.529296875), w: 60.27435302734375, h: 28.2315673828125>,\n",
       "     mask: None,\n",
       "     score: PredictionScore: <value: 0.9451640844345093>,\n",
       "     category: Category: <id: 1, name: person>>,\n",
       " ObjectPrediction<\n",
       "     bbox: BoundingBox: <(307.9460754394531, 563.374755859375, 533.8001708984375, 714.992431640625), w: 225.85409545898438, h: 151.61767578125>,\n",
       "     mask: None,\n",
       "     score: PredictionScore: <value: 0.9997795224189758>,\n",
       "     category: Category: <id: 2, name: bicycle>>,\n",
       " ObjectPrediction<\n",
       "     bbox: BoundingBox: <(2490.4498901367188, 594.3944091796875, 2630.6321411132812, 645.453369140625), w: 140.1822509765625, h: 51.0589599609375>,\n",
       "     mask: None,\n",
       "     score: PredictionScore: <value: 0.7660563588142395>,\n",
       "     category: Category: <id: 2, name: bicycle>>,\n",
       " ObjectPrediction<\n",
       "     bbox: BoundingBox: <(2481.96728515625, 595.0757446289062, 2631.77978515625, 647.2606811523438), w: 149.8125, h: 52.1849365234375>,\n",
       "     mask: None,\n",
       "     score: PredictionScore: <value: 0.9902804493904114>,\n",
       "     category: Category: <id: 3, name: car>>,\n",
       " ObjectPrediction<\n",
       "     bbox: BoundingBox: <(923.2579956054688, 678.829345703125, 983.7462158203125, 761.9851684570312), w: 60.48822021484375, h: 83.15582275390625>,\n",
       "     mask: None,\n",
       "     score: PredictionScore: <value: 0.978654682636261>,\n",
       "     category: Category: <id: 5, name: airplane>>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.object_prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xYFS50K9yWJR"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(img_path,cv2.IMREAD_UNCHANGED)\n",
    "img_converted = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "numpydata = np.asarray(img_converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEBvPIKCyWJS"
   },
   "source": [
    "This custom draw_bounding_boxes( ) utility takes in the image and objec_prediction_list from SAHI’s get_sliced_predictions and draw visually pleasing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5VIsmZc6yWJS"
   },
   "outputs": [],
   "source": [
    "def draw_bounding_boxes(image, object_prediction_list):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font_size = int(min(image.size) * 0.008)  # Adjust font size based on image size\n",
    "    font_path = \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"\n",
    "    font = ImageFont.truetype(font_path, font_size) if os.path.exists(font_path) else ImageFont.load_default()\n",
    "\n",
    "    for prediction in object_prediction_list:\n",
    "        bbox = prediction.bbox.to_xywh()\n",
    "        category_id = prediction.category.id\n",
    "        x, y, w, h = bbox\n",
    "        x1, y1, x2, y2 = x, y, x + w, y + h\n",
    "        color = category_colors.get(category_id, 'white')  # Default to white if category_id is unknown\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color, width=6)\n",
    "        # draw.text((x1, y1 - font_size), str(classes_to_idx[category_id]), fill=color, font=font)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Qjt3cQ6CyWJS"
   },
   "outputs": [],
   "source": [
    "# Draw bounding boxes\n",
    "image_with_bboxes = draw_bounding_boxes(img_pil, result.object_prediction_list)\n",
    "\n",
    "# Define the output path\n",
    "output_directory = 'sahi_ouput_data'\n",
    "output_path = os.path.join(output_directory, f'result_{img_filename}.png')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "# Save the resulting image\n",
    "output_path = f'sahi_ouput_data/result_{img_filename}.png'\n",
    "image_with_bboxes.save(output_path)\n",
    "\n",
    "# Display the image (optional, if running in an environment that supports it)\n",
    "image_with_bboxes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu9JBsvxyWJS"
   },
   "source": [
    "## **Without SAHI or Original Image is Forward Passed**\n",
    "\n",
    "Let’s directly pass the original image by resizing it to train image size of (382,216) without SAHI or without Patch Creation to our fine-tuned Faster R-CNN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H6vNjiQCyWJS",
    "outputId": "046400d3-4a8f-4908-bddd-90e7bea77221"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def get_transform():\n",
    "    transforms = []\n",
    "    transforms.append(Tv2.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(Tv2.ToPureTensor())\n",
    "    return Tv2.Compose(transforms)\n",
    "\n",
    "\n",
    "# Function to show image with bounding boxes and save it to disk\n",
    "def show_image_with_boxes(img, targets, category_colors, classes_to_idx, output_path):\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(img)\n",
    "    boxes = targets['boxes'].cpu().numpy()\n",
    "    labels = targets['labels'].cpu().numpy()\n",
    "    scores = targets['scores'].cpu().numpy()\n",
    "    for bbox, label, score in zip(boxes, labels, scores):\n",
    "        if score >= 0.3:  # Only show boxes with confidence score >= 0.3\n",
    "            w = bbox[2] - bbox[0]\n",
    "            h = bbox[3] - bbox[1]\n",
    "            color = category_colors.get(label, 'gray')  # Use gray for unmapped classes\n",
    "            rect = patches.Rectangle((bbox[0], bbox[1]), w, h, linewidth=2, edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            # ax.text(bbox[0], bbox[1], f'{classes_to_idx[label]}: {score:.2f}', color='white', fontsize=12, bbox=dict(facecolor=color, alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "# Function to predict and visualize the output for a single image\n",
    "def predict_and_visualize(image_path, model, device, category_colors, classes_to_idx, output_path):\n",
    "    # Load and transform the image\n",
    "    transforms = get_transform()\n",
    "\n",
    "    img = PIL.Image.open(image_path).convert(\"RGB\")\n",
    "    img = img.resize((382, 216), PIL.Image.BILINEAR)  # Resize the image\n",
    "    # print(img.size)\n",
    "    img_tensor = F.to_tensor(img)\n",
    "    img_trans = transforms(img_tensor).to(device)\n",
    "\n",
    "\n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(torch.unsqueeze(img_trans,dim=0))[0]\n",
    "\n",
    "    # Visualize and save the output\n",
    "    show_image_with_boxes(img, output, category_colors, classes_to_idx, output_path)\n",
    "\n",
    "model = load_model(checkpoint_path, device)\n",
    "# Path to the image you want to predict\n",
    "\n",
    "img_path = 'test/7882.jpg'\n",
    "img_filename_temp = img_path.split('/')[1]\n",
    "img_filename = img_filename_temp.split('.')[0]\n",
    "\n",
    "\n",
    "# Path to save the output image\n",
    "output_path = f'{img_filename}_fwd_pass_full_size.jpg'\n",
    "\n",
    "# Predict and visualize the output for the single image\n",
    "predict_and_visualize(img_path, model, device, category_colors, classes_to_idx, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSOdGwzryWJS"
   },
   "source": [
    "## Inference Comparison between SAHI and Original Image Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyfRma7ayWJS"
   },
   "source": [
    "## Comparison 1\n",
    "\n",
    "test/7882.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnvehGZRyWJT"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "  <img src=\"https://www.dropbox.com/scl/fi/11i9pi4rbdbi6jkovloce/7882_fwd_pass_full_size.jpg?rlkey=xfjemqbp8lnjd5bdtz5vlgctq&st=st332fpi&dl=1\" alt=\"Image\" >\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjOJZmphyWJT"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "  <img src=\"https://www.dropbox.com/scl/fi/mty3rjdnygiiewvq0r47g/SAHI-7882.png?rlkey=wnvgj58q3xiaq2t44gpec2cvq&st=4ijscb9f&dl=1\" alt=\"Image\" >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBJKmeK-yWJT"
   },
   "source": [
    "## Comparison 2\n",
    "\n",
    "test/2843.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IGy0KH7yWJT"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "  <img src=\"https://www.dropbox.com/scl/fi/r791wuqwu2qz8xbyl9k78/2843_fwd_pass_full_size.jpg?rlkey=gyo8u7tlvgcr5h0yzpwb5i7cf&st=b0gs3xki&dl=1\" alt=\"Image\" >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzmT3GH-yWJT"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "  <img src=\"https://www.dropbox.com/scl/fi/3omc3dyu11tdsvdd2b8is/SAHI-2843.png?rlkey=213f1gazbylucazryjaoxoutw&st=5wrmzp7o&dl=1\" alt=\"Image\" >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIOYPWG8yWJT"
   },
   "source": [
    "## Comparison 3\n",
    "\n",
    "test/1070.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c27GpDucyWJT"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "  <img src=\"https://www.dropbox.com/scl/fi/avh3l9wuq0poumo9962ux/1070_fwd_pass_Full-Size.jpg?rlkey=mpxy3d05ljsdvhzbca9hjj1d6&st=n97vuzj4&dl=1\" alt=\"Image\" >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2GVBSNIyWJT"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "  <img src=\"https://www.dropbox.com/scl/fi/4k1absw5i4ss41mxrez8o/1070-With-SAHI.png?rlkey=dycnkku1w8v3ks0k13ly19rbk&st=ckaiu1dh&dl=1\" alt=\"Image\" >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29ezkE3DyWJT"
   },
   "source": [
    "**The results are pretty impressive indeed!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ultralytics_pytorch",
   "language": "python",
   "name": "ultralytics_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
