{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5175, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Source: https://github.com/Yuxiang1995/ICDAR2021_MFD\n",
    "# Define the distribution_focal_loss function\n",
    "def distribution_focal_loss(pred, label):\n",
    "    r\"\"\"Distribution Focal Loss (DFL) is from `Generalized Focal Loss: Learning\n",
    "    Qualified and Distributed Bounding Boxes for Dense Object Detection\n",
    "    <https://arxiv.org/abs/2006.04388>`_.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted general distribution of bounding boxes\n",
    "            (before softmax) with shape (N, n+1), n is the max value of the\n",
    "            integral set `{0, ..., n}` in paper.\n",
    "        label (torch.Tensor): Target distance label for bounding boxes with\n",
    "            shape (N,).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Loss tensor with shape (N,).\n",
    "    \"\"\"\n",
    "    dis_left = label.long()\n",
    "    dis_right = dis_left + 1\n",
    "    weight_left = dis_right.float() - label\n",
    "    weight_right = label - dis_left.float()\n",
    "    loss = F.cross_entropy(pred, dis_left, reduction='none') * weight_left \\\n",
    "        + F.cross_entropy(pred, dis_right, reduction='none') * weight_right\n",
    "    return loss\n",
    "\n",
    "class DistributionFocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 reduction='mean',\n",
    "                 loss_weight=1.0):\n",
    "        super(DistributionFocalLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.loss_weight = loss_weight\n",
    "\n",
    "    def forward(self,\n",
    "                pred,\n",
    "                target,\n",
    "                weight=None,\n",
    "                avg_factor=None,\n",
    "                reduction_override=None):\n",
    "        assert reduction_override in (None, 'none', 'mean', 'sum')\n",
    "        reduction = (\n",
    "            reduction_override if reduction_override else self.reduction)\n",
    "        loss_cls = self.loss_weight * distribution_focal_loss(\n",
    "            pred,\n",
    "            target)\n",
    "        loss = loss_cls.mean()\n",
    "        return loss\n",
    "\n",
    "# Example inputs\n",
    "N, n = 5, 10  # Assume N samples and max value n for the integral set\n",
    "pred = torch.randn(N, n+1, requires_grad=True)  # Random predictions\n",
    "label = torch.rand(N) * n  # Random target labels in the range [0, n]\n",
    "\n",
    "# Instantiate DistributionFocalLoss and compute loss\n",
    "distribution_focal_loss_instance = DistributionFocalLoss()\n",
    "loss_output = distribution_focal_loss_instance(pred, label)\n",
    "\n",
    "loss_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3341, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Source: https://github.com/gau-nernst/centernet-lightning\n",
    "# Define the QualityFocalLoss class\n",
    "class QualityFocalLoss(nn.Module):\n",
    "    '''Quality Focal Loss. Use logits to improve numerical stability. Generalized Focal Loss: https://arxiv.org/abs/2006.04388\n",
    "    '''\n",
    "    def __init__(self, beta: float = 2, reduction: str = 'sum'):\n",
    "        '''Quality Focal Loss. Default values are from the paper\n",
    "\n",
    "        Args:\n",
    "            beta: control the scaling/modulating factor to reduce the impact of easy examples\n",
    "            reduction: either none, sum, or mean \n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        assert reduction in ('none', 'sum', 'mean')\n",
    "        self.beta = beta\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor):\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        modulating_factor = torch.pow(torch.abs(targets - probs), self.beta)\n",
    "        loss = modulating_factor * ce_loss\n",
    "        \n",
    "        if self.reduction == 'none':\n",
    "            return loss\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(loss)\n",
    "        elif self.reduction == 'mean':\n",
    "            return loss.mean()  # Adjusted to use mean directly for simplicity\n",
    "\n",
    "# Example inputs\n",
    "inputs = torch.randn(5, requires_grad=True)  # Example logits for 5 instances\n",
    "targets = torch.empty(5).random_(2)  # Binary targets for the same instances\n",
    "\n",
    "# Instantiate QualityFocalLoss and compute loss\n",
    "quality_focal_loss = QualityFocalLoss(reduction='mean')  # Using 'mean' for illustration\n",
    "loss_output = quality_focal_loss(inputs, targets)\n",
    "\n",
    "# Print output loss\n",
    "print(loss_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
