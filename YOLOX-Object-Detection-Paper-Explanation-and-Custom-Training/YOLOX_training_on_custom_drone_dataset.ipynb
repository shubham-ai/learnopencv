{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyHS1LdLqkYd"
   },
   "source": [
    "# Training YOLOX on a Custom Drone Dataset\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2022/10/yolox-object-detector-paper-explnation-and-custom-training.gif\" alt=\"yolox\" width=\"1000\">\n",
    "\n",
    "In this notebook, we will cover the following.\n",
    "\n",
    "* How to install YOLOX?\n",
    "* Configuring Training Parameters.\n",
    "* Train YOLOX on a custom Drone dataset.\n",
    "* Evaluate\n",
    "* Inference\n",
    "\n",
    "Check out accompanying blog post [YOLOX Object Detector Paper Explanation and Custom Training](https://learnopencv.com/yolox-object-detector-paper-explanation-and-custom-training/).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zvig5AGHqppJ"
   },
   "source": [
    "## 1. Clone YOLOX Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LxpeC_CI48bK",
    "outputId": "a811d7c5-78d8-4cc0-b911-772a9b74845c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'YOLOX'...\n",
      "remote: Enumerating objects: 1723, done.\u001b[K\n",
      "remote: Total 1723 (delta 0), reused 0 (delta 0), pack-reused 1723\u001b[K\n",
      "Receiving objects: 100% (1723/1723), 6.83 MiB | 1.99 MiB/s, done.\n",
      "Resolving deltas: 100% (1020/1020), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Megvii-BaseDetection/YOLOX.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHY_TDRMqvLM"
   },
   "source": [
    "## 2. Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m729ocYT5BPd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/kukil/Elements/BigVision/learnopencv/YOLOX-Object-Detection-Paper-Explanation-and-Custom-Training/YOLOX\n"
     ]
    }
   ],
   "source": [
    "%cd YOLOX\n",
    "!pip3 install -v -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOzgEnZHq396"
   },
   "source": [
    "## 3. Define Classes\n",
    "Since we are using VOC format, by default there will be 20 classes. It is defined in `yolox/data/datasets/voc_classes.py` file. We will modify it to contain only **drone** class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "asE95i2y5FKJ"
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cHuG9M4c5GjN"
   },
   "outputs": [],
   "source": [
    "%%writetemplate yolox/data/datasets/voc_classes.py\n",
    "\n",
    "VOC_CLASSES = (\n",
    "  \"drone\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VsAyp1705GmP"
   },
   "outputs": [],
   "source": [
    "%%writetemplate yolox/data/datasets/coco_classes.py\n",
    "\n",
    "COCO_CLASSES = (\n",
    "  \"drone\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq1XgkkYratO"
   },
   "source": [
    "## 4. Download Pre-trained YOLOX medium weights\n",
    "\n",
    "Available in [YOLOX GitHub releases](https://github.com/Megvii-BaseDetection/YOLOX/releases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bRuAMjNG5MxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-10-07 19:53:15--  https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_m.pth\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/386811486/a0b0f1ca-0e3c-43e4-829d-d9177f6be5f7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221007%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221007T142316Z&X-Amz-Expires=300&X-Amz-Signature=ff84be2ab30ef410195f72b251d8e3da6ba59aa95776559e883ecbedf0da38a8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=386811486&response-content-disposition=attachment%3B%20filename%3Dyolox_m.pth&response-content-type=application%2Foctet-stream [following]\n",
      "--2022-10-07 19:53:16--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/386811486/a0b0f1ca-0e3c-43e4-829d-d9177f6be5f7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221007%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221007T142316Z&X-Amz-Expires=300&X-Amz-Signature=ff84be2ab30ef410195f72b251d8e3da6ba59aa95776559e883ecbedf0da38a8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=386811486&response-content-disposition=attachment%3B%20filename%3Dyolox_m.pth&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203114461 (194M) [application/octet-stream]\n",
      "Saving to: ‘yolox_m.pth’\n",
      "\n",
      "yolox_m.pth         100%[===================>] 193.70M  17.2MB/s    in 11s     \n",
      "\n",
      "2022-10-07 19:53:29 (17.1 MB/s) - ‘yolox_m.pth’ saved [203114461/203114461]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_m.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QI92-Dy3ribu"
   },
   "source": [
    "## 5. Download Drone Dataset\n",
    "The dataset is in following order.\n",
    "```\n",
    "VOCdevkit\n",
    "  |___VOC2012\n",
    "     |___Annotations\n",
    "     |___ImageSets\n",
    "            |___Main\n",
    "                   |___train.txt\n",
    "                   |___valid.txt\n",
    "     |___JPEGImages\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hJZomZU6rk3V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/kukil/Elements/BigVision/learnopencv/YOLOX-Object-Detection-Paper-Explanation-and-Custom-Training/YOLOX/datasets\n",
      "VOCdevkit.zip       100%[===================>]  50.28M  9.67MB/s    in 7.3s    \n",
      "/media/kukil/Elements/BigVision/learnopencv/YOLOX-Object-Detection-Paper-Explanation-and-Custom-Training/YOLOX\n"
     ]
    }
   ],
   "source": [
    "%cd datasets\n",
    "!wget https://github.com/spmallick/learnopencv/blob/master/YOLOX-Object-Detection-Paper-Explanation-and-Custom-Training/Drone-dataset.zip?raw=true -O VOCdevkit.zip -q --show-progress\n",
    "!unzip -qq VOCdevkit.zip\n",
    "!rm VOCdevkit.zip\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nrAb1Lnq_hR"
   },
   "source": [
    "## 6. Configuring Training Parameters\n",
    "In YOLOX, training params are stored in python scripts called experiment files. Example scripts are available in `exps/example/yolox_x_voc/` directory.\n",
    "\n",
    "These scripts contain definition of the following parameters that we need to modify.\n",
    "\n",
    "* **Network depth and width** : [Check out the blog post for defaults](https://learnopencv.com/yolox-object-detector-paper-explanation-and-custom-training/).\n",
    "* **Number of Epochs**\n",
    "* **Number of classes**\n",
    "* **Augmentation info**\n",
    "* **Path to Training and Validation Dataset**\n",
    "\n",
    "\n",
    "Rest of the defaults are available in `yolox/exp/yolox_base.py`.\n",
    "\n",
    "Note that YOLOX has already set the prefixes for Train and validation data path. It is, <br><br> `datasets/VOCdevkit/VOC` + `str(year)` + `/ImageSets/Main`. \n",
    "\n",
    "Hence, we only need to specify **2012** or **2007**.\n",
    "\n",
    "i.e.,\n",
    "```\n",
    "image_sets=[('2012', 'train)],\n",
    "image_sets=[('2012','valid')],\n",
    "```\n",
    "\n",
    "in the `get_dataloader` and `get_eval_loader` functions respectively.\n",
    "\n",
    "In this notebook, we will go through the training pipeline of the **YOLOX Medium** model. You can easily switch between models using the correct configurations. We have already created the model specific scripts for you. Go ahead and download `exp.py` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Download Exp config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/kukil/Elements/BigVision/learnopencv/YOLOX-Object-Detection-Paper-Explanation-and-Custom-Training/YOLOX/exps\n",
      "custom_exps.zip     100%[===================>]   7.46K  --.-KB/s    in 0.001s  \n",
      "Archive:  custom_exps.zip\n",
      "   creating: ExpConfigs/\n",
      "  inflating: ExpConfigs/yolox_voc_l.py  \n",
      "  inflating: ExpConfigs/yolox_voc_m.py  \n",
      "  inflating: ExpConfigs/yolox_voc_nano.py  \n",
      "  inflating: ExpConfigs/yolox_voc_s.py  \n",
      "  inflating: ExpConfigs/yolox_voc_t.py  \n",
      "/media/kukil/Elements/BigVision/learnopencv/YOLOX-Object-Detection-Paper-Explanation-and-Custom-Training/YOLOX\n"
     ]
    }
   ],
   "source": [
    "# Download experiment config files.\n",
    "%cd exps\n",
    "!wget https://github.com/spmallick/learnopencv/blob/master/YOLOX-Object-Detection-Paper-Explanation-and-Custom-Training/ExpConfigs.zip?raw=true -O custom_exps.zip -qq --show-progress\n",
    "!unzip custom_exps.zip\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have set the parameters in the scripts as follows.\n",
    "\n",
    "* **Depth** : 0.67\n",
    "* **Width** : 0.75\n",
    "* **Epochs** : 300\n",
    "* **Number of Classes** : 1\n",
    "* **Train Data Path** : `image_sets=[('2012', 'train)],`\n",
    "* **Validation Data Path** : `image_sets=[('2012','valid')],`\n",
    "* **Augmentation**\n",
    "    - Mixup: 1.0\n",
    "    - Mosaic: 1.0\n",
    "    - HSV: 1.0\n",
    "    - FLIP: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify number of epochs to 25 using stream editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/kukil/Elements/BigVision/learnopencv/YOLOX-Object-Detection-Paper-Explanation-and-Custom-Training/YOLOX\n"
     ]
    }
   ],
   "source": [
    "%cd YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCH = 25\n",
    "!sed -i -e 's/self.max_epoch = 300/self.max_epoch = {MAX_EPOCH}/g' \"exps/ExpConfigs/yolox_voc_m.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep rest of the params as shown above. Feel free to experiment with the parameters. If you want to create experiment file using the example script from scratch, uncomment the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFze7sKZMx55"
   },
   "outputs": [],
   "source": [
    "# '''Copy experiment config file fpr medium model.'''\n",
    "\n",
    "# !cp ./exps/example/yolox_voc/yolox_voc_s.py ./exps/example/yolox_voc/yolox_voc_m.py\n",
    "\n",
    "# '''Set number of classes.'''\n",
    "# NUM_CLASSES = 1\n",
    "# !sed -i -e 's/self.num_classes = 20/self.num_classes = {NUM_CLASSES}/g' \"exps/example/yolox_voc/yolox_voc_m.py\"\n",
    "\n",
    "# '''Set maximum number of epochs. Warning! Running this cell multiple times will create two lines'''\n",
    "# MAX_EPOCH = 25\n",
    "# !sed -i '/self.max_epoch = 300/self.max_epoch={MAX_EPOCH}' \"yolox/exp/yolox_base.py\"\n",
    "\n",
    "# '''Set network depth for medium model.'''\n",
    "# DEPTH = 0.67\n",
    "# !sed -i -e 's/self.depth = 0.33/self.depth = {DEPTH}/g' \"exps/example/yolox_voc/yolox_voc_m.py\"\n",
    "\n",
    "# '''Set network width for medium model.'''\n",
    "# WIDTH = 0.75\n",
    "# !sed -i -e 's/self.width = 0.50/self.width = {WIDTH}/g' \"exps/example/yolox_voc/yolox_voc_m.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHBBUbb45eDZ"
   },
   "source": [
    "## 7. Train\n",
    "Let's train the YOLOX medium model. For others, change the exp config file path and pre-trained weights path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z4HNUKtt5Ova",
    "outputId": "80d9073c-f97f-4496-8455-c2f41b1a14a1"
   },
   "outputs": [],
   "source": [
    "!python tools/train.py -f exps/ExpConfigs/yolox_voc_m.py -d 1 -b 16 --fp16 -o -c yolox_m.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAYtpgyU5jYm"
   },
   "source": [
    "## 8. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiJFs8sI5irz",
    "outputId": "08d8de00-bdf9-49c4-a9e2-cf33bc287b79"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"YOLOX_outputs/yolox_voc_m/best_ckpt.pth\"\n",
    "!python3 tools/eval.py -c {MODEL_PATH} -b 16 -d 1 --conf 0.001 -f exps/ExpConfigs/yolox_voc_m.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8rhMWrn5pZ1"
   },
   "source": [
    "## 9. Image Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZtR3F5JtEjU",
    "outputId": "4d31b95a-302d-4cd6-b992-bba4ff5f41f5"
   },
   "outputs": [],
   "source": [
    "# Download Images\n",
    "%mkdir inference_media\n",
    "%cd inference_media\n",
    "!wget https://www.dropbox.com/s/1dy29ys1fkce8k3/bird-and-drone.png?dl=1 -O bird-and-drone.jpg -qq --show-progress\n",
    "!wget https://www.dropbox.com/s/i0afm1nqm6iiuji/eagle-capturing-drone.png?dl=1 -O eagle-capturing-drone.jpg -qq --show-progress\n",
    "!wget https://www.dropbox.com/s/kje4h0avj2scgjj/eagle-vs-drone.png?dl=1 -O eagle-vs-drone.jpg -qq --show-progress\n",
    "!wget https://www.dropbox.com/s/jhjy3lfl5908vta/drone-vs-birds.jpg?dl=1 -O drone-vs-birds.jpg -qq --show-progress\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMeT1LPctFl1",
    "outputId": "b8d33a57-b55c-4084-9a4a-18902099a6ba"
   },
   "outputs": [],
   "source": [
    "!python tools/demo.py image -f exps/ExpConfigs/yolox_voc_m.py -c {MODEL_PATH} --path ./inference_media/ --conf 0.25 --nms 0.45 --tsize 640 --save_result --device gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vs0_pZnItXkN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to modify the plots after multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-d1EG3BtfXi"
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for file in glob.glob(\"YOLOX_outputs/yolox_voc_m/vis_res/**/*.jpg\"):\n",
    "  img = cv2.imread(file)\n",
    "  images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "5ULQQaJ5tiZl",
    "outputId": "594c6a9b-8a90-45a0-ba2e-b7bfd7c973ae"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15))\n",
    "for i in range(len(images)):\n",
    "  plt.subplot(1, len(images), i+1); \n",
    "  plt.imshow(images[i][...,::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAAO7t2DvWlR"
   },
   "source": [
    "## 10. Video Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_-s1EOAvkue"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/u1kqu0yxj07e35e/Drones-1-original.mp4?dl=1 -O Drones-1-original.mp4 -qq --show-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPUIeCZLuzYI"
   },
   "outputs": [],
   "source": [
    "!python tools/demo.py video -f exps/ExpConfigs/yolox_voc_m.py -c {MODEL_PATH} --path Drones-1-original.mp4 --conf 0.25 --nms 0.45 --tsize 640 --save_result --device gpu"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "yolox",
   "language": "python",
   "name": "yolox-2-10-22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
