{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c214d8e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook fine-tunes BERT on an Arxix abstract classification dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fec93",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80ea05e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.34.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.14.6)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.24.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.59.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.0)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (68.0.0)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers datasets evaluate accelerate\n",
    "!pip install scikit-learn\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b6459",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3cb528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc09da",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d794fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_PROCS = 32\n",
    "LR = 0.00005\n",
    "EPOCHS = 5\n",
    "MODEL = 'bert-base-uncased'\n",
    "OUT_DIR = 'arxiv_bert'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b35b6b5",
   "metadata": {},
   "source": [
    "## Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d809f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"ccdv/arxiv-classification\", split='train')\n",
    "valid_dataset = load_dataset(\"ccdv/arxiv-classification\", split='validation')\n",
    "test_dataset = load_dataset(\"ccdv/arxiv-classification\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "460cfc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 28388\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 2500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 2500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(valid_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f58ca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Constrained Submodular Maximization via a\\nNon-symmetric Technique\\n\\narXiv:1611.03253v1 [cs.DS] 10 Nov 2016\\n\\nNiv Buchbinder∗\\n\\nMoran Feldman†\\n\\nNovember 11, 2016\\n\\nAbstract\\nThe study of combinatorial optimization problems with a submodular objective has attracted\\nmuch attention in recent years. Such problems are important in both theory and practice because\\ntheir objective functions are very general. Obtaining further improvements for many submodular\\nmaximization problems boils down to finding better algorithms for optimizing a relaxation of\\nthem known as the multilinear extension.\\nIn this work we present an algorithm for optimizing the multilinear relaxation whose guarantee improves over the guarantee of the best previous algorithm (which was given by Ene\\nand Nguyen (2016)). Moreover, our algorithm is based on a new technique which is, arguably,\\nsimpler and more natural for the problem at hand. In a nutshell, previous algorithms for this\\nproblem rely on symmetry properties which are natural only in the absence of a constraint. Our\\ntechnique avoids the need to resort to such properties, and thus, seems to be a better fit for\\nconstrained problems.\\n\\n∗\\n\\nDepartment of Statistics and Operations Research, School of Mathematical Sciences, Tel Aviv university, Israel.\\nEmail: niv.buchbinder@gmail.com.\\n†\\nDepart. of Mathematics and Computer Science, The Open University of Israel. Email: moranfe@openu.ac.il.\\n\\n\\x0c1\\n\\nIntroduction\\n\\nThe study of combinatorial optimization problems with a submodular objective has attracted much\\nattention in recent years. Such problems are important in both theory and practice because their\\nobjective functions are very general—submodular functions generalize, for example, cuts functions\\nof graphs and directed graphs, the mutual information function, matroid weighted rank functions and log-determinants. More specifically, from a theoretical perspective, many well-known\\nproblems in combinatorial optimization are in fact submodular maximization problems, including:\\nMax-Cut [30, 33, 38, 40, 56], Max-DiCut [20, 30, 31], Generalized Assignment [10, 14, 22, 27],\\nMax-k-Coverage [19, 41], Max-Bisection [3, 28] and Facility Location [1, 16, 17]. From a practical\\nperspective, submodular maximization problems have found uses in social networks [32, 39], vision [5, 36], machine learning [43, 44, 45, 49, 50] and many other areas (the reader is referred, for\\nexample, to a comprehensive survey by Bach [4]).\\nThe techniques used by approximation algorithms for submodular maximization problems usually fall into one of two main approaches. The first approach is combinatorial in nature, and is\\nmostly based on local search techniques and greedy rules. This approach has been used as early\\nas the late 70’s for maximizing a monotone submodular function subject to a matroid constraint\\n(some of these works apply only to specific types of matroids) [15, 26, 34, 35, 37, 42, 53, 54]. Later\\nworks used this approach to handle also problems with non-monotone submodular objective functions and different constraints [6, 21, 25, 47, 48], yielding in some cases optimal algorithms [6, 55].\\nHowever, algorithms based on this approach tend to be highly tailored for the specific structure of\\nthe problem at hand, making extensions quite difficult.\\nThe second approach used by approximation algorithms for submodular maximization problems overcomes the above obstacle. This approach resembles a common paradigm for designing\\napproximation algorithms and involves two steps. In the first step a fractional solution is found for\\na relaxation of the problem, known as the multilinear relaxation. In the second step the fractional\\nsolution is rounded to obtain an integral one while incurring a bounded loss in the objective. This\\napproach has been used to obtain improved approximations for many problems [8, 11, 12, 24, 46].\\nVarious techniques have been developed for rounding the fractional solution. These techniques\\ntend to be quite flexible, and usually can extend to many related problem. In particular, the\\nContention Resolution Schemes framework of [12] yields a rounding procedure for every constraint\\nwhich can be presented as the intersection of a few basic constraints such as knapsack constraints,\\nmatroid constraints and matching constraints. Given this wealth of rounding procedures, obtaining\\nfurther improvements for many important submodular maximization problems (such as maximizing\\na submodular function subject to a matroid or knapsack constraint) boils down to obtaining improved algorithms for finding a good fractional solution, i.e., optimizing the multilinear relaxation.\\n\\n1.1\\n\\nMaximizing the Multilinear Relaxation\\n\\nAt this point we would like to present some terms more formally. A submodular function is a set\\nfunction f : 2N → R obeying f (A) + f (B) ≥ f (A ∪ B) + f (A ∩ B) for any sets A, B ⊆ N . A\\nsubmodular maximization problem is the problem of finding a set S ⊆ N maximizing f subject to\\nsome constraint. Formally, let I be the set of subsets of N obeying the constraint. Then, we are\\ninterested in the following problem.\\nmax f (A)\\ns.t. A ∈ I ⊆ 2N\\nA relaxation of the above problem replaces I with a polytope P ⊆ [0, 1]N containing the\\n1\\n\\n\\x0ccharacteristic vectors of all the sets of I. In addition, a relaxation must replace the function f with\\nan extension function F : [0, 1]N → R. Thus, a relaxation is a fractional problem of the following\\nformat.\\nmax F (x)\\ns.t. x ∈ P ⊆ [0, 1]N\\nDefining the “right” extension function, F , for the relaxation is a challenge, as, unlike the linear\\ncase, there is no single natural candidate. The objective that turned out to be useful, and is, thus,\\nused by multilinear relaxation is known as the multilinear extension (first introduced by [8]). The\\nvalue F (x) of this extension for any vector x ∈ [0, 1]N is defined as the expected value of f over\\na random subset R(x) ⊆ N containing every element u ∈ N independently with probability xu .\\nFormally, for every x ∈ [0, 1]N ,\\nY\\nX\\nY\\n(1 − xu ) .\\nF (x) = E[R(x)] =\\nf (S)\\nxu\\nS⊆N\\n\\nu∈S\\n\\nu∈S\\n/\\n\\nThe first algorithm for optimizing the multilinear relaxation was the Continuous Greedy algorithm designed by Calinescu et al. [8]. When the submodular function f is non-negative and\\nmonotone1 and P is solvable2 this algorithm finds a vector x ∈ P such that E[F (x)] ≥ (1 − 1/e −\\no(1)) · f (OP T ) (where OP T is the set maximizing f among all sets whose characteristic vectors\\nbelongs to P ). Interestingly, the guarantee of Continuous Greedy is optimal for monotone functions\\neven when P is a simple cardinality constraint [8, 53].\\nOptimizing the multilinear relaxation when f is not necessarily monotone proved to be a more\\nchallenging task. Initially, several algorithms for specific polytopes were suggested [29, 47, 57].\\nLater on, improved general algorithms were designed that work whenever f is non-negative and\\nP is down-closed3 and solvable [13, 24]. Designing algorithms that work in this general setting is\\nhighly important as many natural constraints fall into this framework. Moreover, the restriction of\\nthe algorithms to down-closed polytopes is unavoidable as Vondrák [57] proved that no algorithm\\ncan produce a vector x ∈ P obeying E[F (x)] ≥ c · f (OP T ) for any constant c > 0 when P is\\nsolvable but not down-closed.\\nUp until recently, the best algorithm for this general setting was called Measured Continuous\\nGreedy [24]. It guaranteed to produce a vector x ∈ P obeying E[F (x)] ≥ (1/e − o(1)) · f (OP T ) ≈\\n0.367 · f (OP T ) [24]. The natural feel of the guarantee of Measured Continuous Greedy and the fact\\nthat it was not improved for a few years made some people suspect that it is optimal. Recently,\\nan evidence against this conjecture was given by [7], which described an algorithm for the special\\ncase of a cardinality constraint with an improved approximation guarantee of 0.371. Even more\\nrecently, Ene and Nguyen [18] shuttered the conjecture completely. By extending the technique\\nused by [7], they showed that one can get an approximation guarantee 0.372 for every down-closed\\nand solvable polytope P . On the inapproximability side, Oveis Gharan and Vondrák [29] proved\\nthat no algorithm can achieve approximation better than 0.478 even when P is the matroid polytope\\nof a partition matroid. Closing the gap between the best algorithm and inapproximability result\\nfor this fundamental problem remains an important open problem.\\n1\\n\\nA\\nA\\n3\\nA\\nupper\\n2\\n\\nset function f : 2N → R is monotone if f (A) ≤ f (B) for every A ⊆ B ⊆ N .\\npolytope is solvable if one can optimize linear functions over it.\\npolytope P ⊆ [0, 1]N is down-closed if y ∈ P implies that every vector x ∈ [0, 1]N which is coordinate-wise\\nbounded by y must belong to P as well.\\n\\n2\\n\\n\\x0c1.2\\n\\nOur Contribution\\n\\nOur main contribution is an algorithm with an improved guarantee for maximizing the multilinear\\nrelaxation.\\nTheorem 1.1. There exists a polynomial time algorithm that given a non-negative submodular\\nfunction f : 2N → R≥0 and a solvable down-closed polytope P ⊆ [0, 1]N finds a vector x ∈ P\\nobeying F (x) ≥ 0.385 · f (OP T ), where OP T = arg max{f (S) : 1S ∈ P } and F is the multilinear\\nextension of f .\\nAdmittedly, the improvement in the guarantee obtained by our algorithm compared to the\\n0.372 guarantee of [18] is relatively small. However, the technique underlying our algorithm is\\nvery different, and, arguably, much cleaner, than the technique underlying the previous results\\nimproving over the natural guarantee of 1/e [7, 18]. Moreover, we believe our technique is more\\nnatural for the problem at hand, and thus, is likely to yield further improvements in the future. In\\nthe rest of this section we explain the intuition on which we base this belief.\\nThe results of [7, 18] are based on the observation that the guarantee of Measured Continuous\\nGreedy improves when the algorithm manages to increase all the coordinates of its solution at a\\nslow rate. Based on this observation, [7, 18] run an instance of Measured Continuous Greedy (or a\\ndiscretized version of it), and force it to raise the coordinates slowly. If this extra restriction does\\nnot affect the behavior of the algorithm significantly, then it produces a solution with an improved\\nguarantee. Otherwise, [7, 18] argue that the point in which the extra restriction affect the behavior\\nof Measured Continuous Greedy reveals a vector x ∈ P which contains a significant fraction of\\nOP T . Once x is available, one can use the technique of unconstrained submodular maximization,\\ndescribed by [6], that has higher approximation guarantee of 1/2 > 1/e, to extract from x a vector\\n0 ≤ y ≤ x of large value. The down-closeness of P guarantees that y belongs to P as well.\\nUnfortunately, the use of the unconstrained submodular maximization technique in the above\\napproach is very problematic for two reasons. First, this technique is based on ideas that are\\nvery different from the ideas used by the analysis of Measured Continuous Greedy. This makes\\nthe combination of the two quite involved. Second, on a more abstract level, the unconstrained\\nsubmodular maximization technique is based on a symmetry which exists in the absence of a\\nconstraint since f¯(S) = f (N \\\\ S) is non-negative and submodular whenever f has these properties.\\nHowever, this symmetry breaks when a constraint is introduced, and thus, the unconstrained\\nsubmodular maximization technique does not seem to be a good fit for a constrained problem.\\nOur algorithm replaces the symmetry based unconstrained submodular maximization technique\\nwith a local search algorithm. More specifically, it first executes the local search algorithm. If the\\noutput of the local search algorithm is good, then our algorithm simply returns it. Otherwise, we\\nobserve that the poor value of the output of the local search algorithm guarantees that it is also\\nfar from OP T in some sense. Our algorithm then uses this far from OP T solution to guide an\\ninstance of Measured Continuous Greedy, and help it avoid bad decisions.\\nAs it turns out, the analysis of Measured Continuous Greedy and the local search algorithm\\nuse similar ideas and notions. Thus, the two algorithms combine quite cleanly, as can be observed\\nfrom Section 3.\\n\\n3\\n\\n\\x0c2\\n\\nPreliminaries\\n\\nOur analysis uses another useful extension of submodular functions. Given a submodular function\\nf : 2N → R, its Lovász extension is a function fˆ: [0, 1]N → R defined by\\nfˆ(x) =\\n\\nZ\\n\\n1\\n\\nf (Tλ (x))dλ ,\\n\\n0\\n\\nwhere Tλ (x) = {u ∈ N : xu < λ}. The Lovász extension has many important applications (see,\\ne.g., [9, 52]), however, in this paper we only use it in the context of the following known result\\n(which is an immediate corollary of the work of [51]).\\nLemma 2.1. Given the multilinear extension F and the Lovász extension fˆ of a submodular function f : 2N → R, it holds that F (x) ≥ fˆ(x) for every vector x ∈ [0, 1]N .\\nWe now define some additional notation that we use. Given a set S ⊆ N and an element u ∈ N ,\\nwe denote by 1S and 1u the characteristic vectors of the sets S and {u}, respectively, and by S + u\\nand S − u the sets S ∪ {u} and S \\\\ {u}, respectively. Given two vectors x, y ∈ [0, 1]N , we denote\\nby x ∨ y, x ∧ y and x ◦ y the coordinate-wise maximum, minimum and multiplication, respectively,\\nof x and y.4 Finally, given a vector x ∈ [0, 1]N and an element u ∈ N , we denote by ∂u F (x) the\\nderivative of F with respect to u at the point x. The following observation gives a simple formula\\nfor ∂u F (x). This observation holds because F is a multilinear function.\\nObservation 2.2. Let F (x) be the multilinear extension of a submodular function f : 2N → R.\\nThen, for every u ∈ N and x ∈ [0, 1]N ,\\n(1 − xu ) · ∂u F (x) = F (x ∨ 1u ) − F (x) .\\nIn the rest of the paper we assume, without loss of generality, that 1u ∈ P for every element\\nu ∈ N and that n is larger than any given constant. The first assumption is justified by the\\nobservation that every element u violating this assumption can be safely removed from N since it\\ncannot belong to OP T . The second assumption is justified by the observation that it is possible to\\nfind a set S obeying 1S ∈ P and f (S) = f (OP T ) in constant time when n is a constant.\\nAnother issue that needs to be kept in mind is the representation of submodular functions. We\\nare interested in algorithms whose time complexity is polynomial in |N |. However, the representation of the submodular function f might be exponential in this size; thus, we cannot assume that\\nthe representation of f is given as part of the input for the algorithm. The standard way to bypass\\nthis difficulty is to assume that the algorithm has access to f through an oracle. We assume the\\nstandard value oracle that is used in most of the previous works on submodular maximization.\\nThis oracle returns, given any subset S ⊆ N , the value f (S).\\n\\n3\\n\\nMain Algorithm\\n\\nIn this section we present the algorithm used to prove Theorem 1.1. This algorithm uses two\\ncomponents. The first component is a close variant of a fractional local search algorithm suggested\\nby Chekuri et al. [13] which has the following properties.\\n4\\n\\nMore formally, for every element u ∈ N , (x ∨ y)u = max{xu , yu }, (x ∧ y)u = min{xu , yu } and (x ◦ y)u = xu · yu .\\n\\n4\\n\\n\\x0cLemma 3.1 (Follows from Chekuri et al. [13]). There exists a polynomial time algorithm which\\nreturns vector x ∈ P such that, with high probability, for every vector y ∈ P ,\\n1\\n1\\nF (x) ≥ F (x ∧ y) + F (x ∨ y) − o(1) · f (OP T ) .\\n2\\n2\\n\\n(1)\\n\\nProof. Let M = max{f (u), f (N − u) : u ∈ N }, and let a be an arbitrary constant larger than 3.\\nThen, Lemmata 3.7 and 3.8 of Chekuri et al. [13] imply that, with high probability, the fractional\\nlocal search algorithm they suggest terminates in polynomial time and outputs a vector x ∈ P\\nobeying, for every vector y ∈ P ,\\n2F (x) ≥ F (x ∧ y) + F (x ∨ y) −\\n\\n5M\\n.\\nna−2\\n\\nMoreover, the output vector x is in P whenever the fractional local search algorithm terminates.\\nOur assumption that 1u ∈ P for every element u ∈ N implies, by submodularity, that f (S) ≤\\nn · f (OP T ) for every set S ⊆ N . Since M is the maximum over values of f , we get also M ≤\\nn · f (OP T ). Using this observation, and plugging a = 4, we get that there exists an algorithm\\nwhich, with high probability, terminates after T (n) operations (for some polynomial function T (n))\\nT)\\nfor every vector y ∈ P .\\nand outputs a vector x ∈ P obeying 2F (x) ≥ F (x ∧ y) + F (x ∨ y) − 5·f (OP\\nn\\nMoreover, the output vector x belongs to P whenever the algorithm terminates.\\nTo complete the lemma, we consider a procedure that executes the above algorithm for T (n)\\noperations, and return its output if it terminates within this number of operations. If the algorithm\\nfails to terminate within this number of operations, which happens with a diminishing probability,\\nthen the procedure simply returns 1∅ (which always belongs to P since P is down-closed). One\\ncan observe that this procedure has all the properties guaranteed by the lemma.\\nThe second component of our algorithm is a new auxiliary algorithm which we present and\\nanalyze in Section 4. This auxiliary algorithm is the main technical contribution of this paper, and\\nits guarantee is given by the following theorem.\\nTheorem 3.2. There exists a polynomial time algorithm that given a vector z ∈ [0, 1]N and a value\\nts ∈ [0, 1] outputs a vector x ∈ P obeying\\nE[F (x)] ≥ ets −1 · [(2 − ts − e−ts − o(1)) · f (OP T ) − (1 − e−ts ) · F (z ∧ 1OP T )\\n−ts\\n\\n− (2 − ts − 2e\\n\\n(2)\\n\\n) · F (z ∨ 1OP T )] .\\n\\nOur main algorithm executes the algorithms suggested by Lemma 3.1 followed by the algorithm\\nsuggested by Theorem 3.2. Notice that the second of these algorithms has two parameters in\\naddition to f and P : a parameter z which is set to be the output of the first algorithm, and a\\nparameter ts which is set to be a constant to be determined later. After the two above algorithms\\nterminate, our algorithm returns the output of the first algorithm with probability p, for a constant\\np to be determined later, and with the remaining probability it returns the output of the second\\nalgorithm.5 A formal description of our algorithm is given as Algorithm 1. Observe that Lemma 3.1\\nand Theorem 3.2 imply together that Algorithm 1 is a polynomial time algorithm which always\\noutputs a vector in P .\\nTo prove Theorem 1.1, it remains to analyze the quality of the solution produced by Algorithm 1.\\n5\\n\\nClearly it is always better to return the better of the two solution instead of randomizing between them. However,\\ndoing so will require the algorithm to either have an oracle access to F or estimate the values of the solutions using\\nsampling (the later can be done using standard techniques—see, e.g., [8]). For the sake of simplicity, we chose here\\nthe easier to analyze approach of randomizing between the two solutions.\\n\\n5\\n\\n\\x0cAlgorithm 1: Main Algorithm(f, P )\\n1\\n2\\n3\\n\\nExecute the algorithm suggested by Lemma 3.1, and let x1 ∈ P be its output.\\nExecute the algorithm suggested by Theorem 3.2 with z = x1 , and let x2 be its output.\\nreturn with probability p the solution x1 , and the solution x2 otherwise.\\n\\nLemma 3.3. When its parameters are set to ts = 0.372 and p = 0.23, Algorithm 1 produces a\\nsolution whose expected value is at least 0.385 · f (OP T ).\\nProof. Let E be the event that x1 , the output of the algorithm suggested by Lemma 3.1, satisfies\\nInequality (1). Since E is a high probability event, it is enough to prove that, conditioned on E,\\nAlgorithm 1 produces a solution whose expected value is at least c · f (OP T ) for some constant\\nc > 0.385. The rest of the proof of the lemma is devoted to proving the last claim. Throughout it,\\neverything is implicitly conditioned on E.\\nAs we are conditioning on E, we can plug y = 1OP T and, respectively, y = x1 ∧ 1OP T into\\nInequality (1) to get\\n1\\n1\\nF (x1 ) ≥ F (x1 ∧ 1OP T ) + F (x1 ∨ 1OP T ) − o(1) · f (OP T )\\n(3)\\n2\\n2\\nand\\nF (x1 ) ≥ F (x1 ∧ 1OP T ) − o(1) · f (OP T ) ,\\n(4)\\nwhere the last inequality follows by noticing that x1 ∨ (x1 ∧ 1OP T ) = x1 . Next, let E[F (x2 ) | x1 ]\\ndenote the expected value of F (x2 ) conditioned on the given value of x1 . Inequality (2) guarantees\\nthat\\nE[F (x2 ) | x1 ] ≥ ets −1 · [(2 − ts − e−ts − o(1)) · f (OP T ) − (1 − e−ts ) · F (x1 ∧ 1OP T )\\n−ts\\n\\n− (2 − ts − 2e\\n\\n(5)\\n\\n) · F (x1 ∨ 1OP T )] .\\n\\nRecall that Algorithm 1 returns x1 with probability p, and x2 otherwise. Hence, the expected\\nvalue of its output is\\nE[p · F (x1 ) + (1 − p) · E[F (x2 ) | x1 ]] ,\\n(6)\\nwhere the expectation is over x1 .\\nOptimizing the constants. We would like to derive from Inequalities (3), (4) and (5) the best\\nlower bound we can get on (6). To this end, let p1 and p2 be two non-negative numbers such that\\np1 + p2 = p, and let p3 = 1 − p. Using the above inequalities and this notation, (6) can now be\\nlower bounded by\\n\\x15\\n\\x14\\n1\\n1\\np1 · E[F (x1 ∧ 1OP T )] + E[F (x1 ∨ 1OP T )] − o(1) · f (OP T )\\n2\\n2\\n+ p2 · [E[F (x1 ∧ 1OP T )] − o(1) · f (OP T )]\\n+ p3 · ets −1 · [(2 − ts − e−ts − o(1)) · f (OP T ) − (1 − e−ts ) · E[F (x1 ∧ 1OP T )]\\n− (2 − ts − 2e−ts ) · E[F (x1 ∨ 1OP T )]] ,\\nwhich can be rewritten as\\n\\x10p\\n\\n\\x11\\n+ p2 − p3 · ets −1 (1 − e−ts ) · E[F (x1 ∧ 1OP T )]\\n\\x11\\n\\x10 p2\\n1\\n− p3 · ets −1 (2 − ts − 2e−ts ) · E[F (x1 ∨ 1OP T )]\\n+\\n2\\n+ p3 · ets −1 (2 − ts − e−ts ) · f (OP T ) − o(1) · f (OP T ) .\\n1\\n\\n6\\n\\n\\x0cTo get the most out of this lower bound we need to maximize the coefficient of f (OP T ) while\\nkeeping the coefficients of E[F (x1 ∧ 1OP T )] and E[F (x1 ∨ 1OP T )] non-negative (so that they can\\nbe ignored due to non-negativity of f ). This objective is formalized by the following non-convex\\nprogram.\\nmax p3 · ets −1 (2 − ts − e−ts )\\ns.t. p1 /2 + p2 − p3 · ets −1 (1 − e−ts )\\np1 /2 − p3 · ets −1 (2 − ts − 2e−ts )\\np1 + p2 + p3\\np 1 , p 2 , p 3 , ts\\n\\n≥0\\n≥0\\n=1\\n≥0\\n\\nSolving the program, we get that the best solution is approximately p1 = 0.205, p2 = 0.025,\\np3 = 0.770 and ts = 0.372, and the objective function value corresponding to this solution is at\\nleast 0.3856. Hence, we have managed to lower bound (6) (and thus, also the expected value of the\\noutput of Algorithm 1) by 0.3856 · f (OP T ) for p = 0.23 and ts = 0.372, which completes the proof\\nof the lemma.\\n\\n4\\n\\nAided Measured Continuous Greedy\\n\\nIn this section we present the algorithm used to prove Theorem 3.2. Proving the above theorem\\ndirectly is made more involved by the fact that the vector z might be fractional. Instead, we prove\\nthe following simplified version of Theorem 3.2 for integral values, and show that the simplified\\nversion implies the original one.\\nTheorem 4.1. There exists a polynomial time algorithm that given a set Z ⊆ N and a value\\nts ∈ [0, 1] outputs a vector x ∈ P obeying\\nE[F (x)] ≥ ets −1 · [(2 − ts − e−ts − o(1)) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] .\\nNext is the promised proof that Theorem 4.1 implies Theorem 3.2.\\nProof of Theorem 3.2 given Theorem 4.1. Consider an algorithm ALG that given the z and ts arguments specified by Theorem 3.2 executes the algorithm guaranteed by Theorem 4.1 with the same\\nvalue ts and with a random set Z distributed like R(z). The output of ALG is then the output\\nproduced by the algorithm guaranteed by Theorem 4.1. Let us denote this output by x.\\nTheorem 4.1 guarantees that, for every given Z,\\nE[F (x) | Z] ≥ ets −1 · [(2 − ts − e−ts − o(1)) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] .\\nTo complete the proof we take the expectation over Z over the two sides of the last inequality and\\nobserve that\\nE[f (Z ∩ OP T )] = E[f (R(z) ∩ OP T )] = E[f (R(z ∧ 1OP T ))] = F (z ∧ 1OP T )\\nand\\nE[f (Z ∪ OP T )] = E[f (R(z) ∪ OP T )] = E[f (R(z ∨ 1OP T ))] = F (z ∨ 1OP T ) .\\n\\n7\\n\\n\\x0cIn the rest of this section we give a non-formal proof of Theorem 4.1. This proof explains the\\nmain ideas necessary for proving the theorem, but uses some non-formal simplifications such as\\nallowing a direct oracle access to the multilinear extension F and giving the algorithm in the form\\nof a continuous time algorithm (which cannot be implemented on a discrete computer). There\\nare known techniques for getting rid of these simplifications (see, e.g., [8]), and a formal proof of\\nTheorem 4.1 based on these techniques is given in Appendix A.\\nThe algorithm we use for the non-formal proof of Theorem 4.1 is given as Algorithm 2. This\\nalgorithm starts with the empty solution y(0) = 1∅ at time 0, and grows this solution over time\\nuntil it reaches the final solution y(1) at time 1. The way the solution grows varies over time.\\nDuring the time range [ts , 1) the solution grows like in the Measured Continuous Greedy algorithm\\nof [24]. On the other hand, during the earlier time range of [0, ts ) the algorithm pretends that the\\nelements of Z do not exist (by giving them negative marginal profits), and grows the solution in\\nthe way Measured Continuous Greedy would have grown it if it was given the ground set N \\\\ Z.\\nThe value ts is the time in which the algorithm switches between the two ways it uses to grow its\\nsolution, thus, the s in the notation ts stands for “switch”.\\nAlgorithm 2: Aided Measured Continuous Greedy (non-formal)(f, P, Z, ts )\\n1\\n2\\n3\\n4\\n5\\n6\\n\\nLet y(0) ← 1∅ .\\nforeach t ∈ [0, 1) do\\nFor each u ∈ N let wu (t) ← F (y(t) ∨ 1u ) − F (y(t)).\\nP\\nP\\n\\x1a\\narg maxx∈P { u∈N \\\\Z wu (t) · xu (t) − u∈Z xu (t)} if t ∈ [0, ts ) ,\\n\\x08P\\nLet x(t) ←\\narg maxx∈P\\nif t ∈ [ts , 1) .\\nu∈N wu (t) · xu (t)\\nIncrease y(t) at a rate of\\n\\ndy(t)\\ndt\\n\\n= (1N − y(t)) ◦ x(t).\\n\\nreturn y(1).\\nWe first note that algorithm outputs a vector in P .\\n\\nObservation 4.2. y(1) ∈ P .\\nProof. Observe that x(t) ∈ P at eachR time t, which implies that (1N − y(t)) · x(t) is also in P since\\n1\\nP is down-closed. Therefore, y(1) = 0 (1N − y(t)) · x(t)dt is a convex combination of vectors in P ,\\nand thus, belongs to P .\\nThe following lemma lower bounds the increase in F (y(t)) as a function of t.\\nLemma 4.3. For every t ∈ [0, 1),\\n(\\nF (y(t) ∨ 1OP T \\\\Z ) − F (y(t))\\ndF (y(t))\\n≥\\ndt\\nF (y(t) ∨ 1OP T ) − F (y(t))\\n\\nif t ∈ [0, ts ) ,\\nif t ∈ [ts , 1) .\\n\\nProof. By the chain rule,\\n!\\n!\\nX\\nX dyu (t) ∂F (y)\\n∂F (y)\\ndF (y(t))\\n=\\n(1 − yu (t)) · xu (t) ·\\n=\\n·\\ndt\\ndt\\n∂yu y=y(t)\\n∂yu y=y(t)\\nu∈N\\nu∈N\\nX\\nX\\n=\\n(xu (t) · [F (y(t) ∨ 1u ) − F (y(t))]) =\\nxu (t) · wu (t) = x(t) · w(t) .\\nu∈N\\n\\n(7)\\n\\nu∈N\\n\\nConsider first the case\\nthis time period Algorithm 2 chooses x(t) as the\\nP t ∈ [0, ts ). During P\\nvector in P maximizing u∈N \\\\Z wu (t) · xu (t) − u∈Z xu (t). Since P is down-closed x(t) = 1OP T \\\\Z\\n8\\n\\n\\x0cis in P and has value 1OP T \\\\Z · w(t) and thus, we have x(t) · w(t) ≥ 1OP T \\\\Z · w(t). Plugging this\\nobservation into Equality (7) yields\\ndF (y(t))\\n= x(t) · w(t) ≥ 1OP T \\\\Z · w(t) =\\ndt\\n≥ F (y(t) ∨ 1OP T \\\\Z ) − F (y(t)) ,\\n\\nX\\n\\n[F (y(t) ∨ 1u ) − F (y(t))]\\n\\nu∈OP T \\\\Z\\n\\nwhere the last inequality holds by the submodularity of f .\\nSimilarity, when t ∈ [ts , 1) Algorithm 2 chooses x(t) as the vector in P maximizing x(t) · w(t).\\nSince 1OP T ∈ P , we get this time x(t) · w(t) ≥ 1OP T · w(t). Plugging this observation into\\nEquality (7) yields\\nX\\ndF (y(t))\\n= x(t) · w(t) ≥ 1OP T · w(t) =\\n[F (y(t) ∨ 1u ) − F (y(t))]\\ndt\\nu∈OP T\\n\\n≥ F (y(t) ∨ 1OP T ) − F (y(t)) ,\\n\\nwhere the last inequality holds again by the submodularity of f .\\nLemma 4.4. For every time t ∈ [0, 1) and set A ⊆ N it holds that\\n\\x11\\n\\x10\\nF (y(t) ∨ 1A ) ≥ e− max{0,t−ts } − e−t max {0, f (A) − f (A ∪ Z)} + e−t · f (A) .\\n\\nProof. First, we note that for every time t ∈ [0, 1] and element u ∈ N ,\\n(\\n1 − e−t\\nif u 6∈ Z ,\\nyu (t) ≤\\n−\\nmax{0,t−t\\ns}\\n1−e\\nif u ∈ Z .\\n\\n(8)\\n\\nThis follows for the following reason. Since x(t) is always in P ⊆ [0, 1]N , yu (t) obeys the\\ndifferential inequality\\ndy(t)\\n= (1 − yu (t)) · x(t) ≤ (1 − yu (t)) .\\ndt\\nUsing the initial condition yu (0) = 0, the solution for this differential inequality is yu (t) ≤ 1 − e−t .\\nTo get the tighter bound for u ∈ Z, we note that at every time t ∈ [0, ts ) Algorithm 2 chooses as\\nx(t) a vector maximizing a linear function in P which assigns a negative weight to elements of Z.\\nSince P is down-closed this maximum must have xu (t) = 0 for every element u ∈ Z. This means\\nthat yu (t) = 0 whenever u ∈ Z and t ∈ [0, ts ]. Moreover, plugging the improved initial condition\\nyu (ts ) = 0 into the above differential inequality yields the promised tighter bound also for the range\\n(ts , 1].\\nNext, let fˆ be the Lovász extension of f . Then, by Lemma 2.1,\\nZ 1\\nˆ\\nf (Tλ (y(t) ∨ 1A ))dλ\\nF (y(t) ∨ 1A ) ≥ f (y(t) ∨ 1A ) =\\n0\\n\\n≥\\n\\nZ\\n\\n1−e−t\\n\\n1−e− max{0,t−ts }\\nZ 1−e−t\\n\\nf (Tλ (y(t) ∨ 1A ))dλ +\\n\\nZ\\n\\n1\\n\\n1−e−t\\n\\nf (Tλ (y(t) ∨ 1A ))dλ\\n\\nf (Tλ (y(t) ∨ 1A ))dλ + e−t · f (A)\\n\\x11\\n\\x10\\n≥ e− max{0,t−ts } − e−t max {0, f (A) − f (A ∪ Z)} + e−t · f (A) .\\n\\n=\\n\\n(9)\\n(10)\\n\\n1−e− max{0,t−ts }\\n\\n9\\n\\n(11)\\n\\n\\x0cInequality (9) follows by the non-negativity of f . Equality (10) follows since, for λ ∈ [1 − e−t , 1),\\nInequality (8) guarantees that yu (t) ≤ λ for every u ∈ N , and thus, Tλ (y(t) ∨ 1A ) = A. Finally\\nInequality (11) follows since, for λ ∈ [1 − e− max{0,t−ts } , 1 − e−t ), Inequality (8) guarantees that\\nyu (t) ≤ λ for every u ∈ Z, and thus, Tλ (y(t) ∨ 1A ) = B(λ) ∪ A for some B(λ) ⊆ N \\\\ Z. By the\\nnon-negativity of f , f (B(λ) ∪ A) ≥ 0. Also, by the submodularity and non-negativity of f , for\\nevery such set B(λ)\\nf (B(λ) ∪ A) ≥ f (A) + f (B(λ) ∪ Z ∪ A) − f (Z ∪ A) ≥ f (A) − f (Z ∪ A) .\\nPlugging the results of Lemma 4.4 into the lower bound given by Lemma 4.3 on the improvement\\nin F (y(t)) as a function of t yields immediately the useful lower bound given by the next corollary.6\\nCorollary 4.5. For every t ∈ [0, 1),\\n(\\nf (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T ) − F (y(t))\\ndF (y(t))\\n≥\\ndt\\nets −t · f (OP T ) − (ets −t − e−t ) · f (Z ∪ OP T ) − F (y(t))\\n\\nif t ∈ [0, ts ) ,\\nif t ∈ [ts , 1) .\\n\\nUsing the last corollary we can complete the proof of Theorem 4.1.\\nProof of Theorem 4.1. We have already seen that y(1)—the output of Algorithm 2—belongs to P .\\nIt remains to show that\\nF (y(1)) ≥ ets −1 · [(2 − ts − e−ts ) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] .\\nCorollary 4.5 describes a differential inequality for F (y(t)). Given the boundary condition\\nF (y(0)) ≥ 0, the solution for this differential inequality within the range t ∈ [0, ts ] is\\nF (y(t)) ≥ (1 − e−t ) · f (OP T \\\\ Z) − (1 − e−t − te−t ) · f (Z ∪ OP T ) .\\nPlugging t = ts into the last inequality, we get\\nF (y(ts )) ≥ (1 − e−ts ) · f (OP T \\\\ Z) − (1 − e−ts − ts e−ts ) · f (Z ∪ OP T ) .\\nLet v = (1 − e−ts ) · f (OP T \\\\ Z) − (1 − e−ts − ts e−ts ) · f (Z ∪ OP T ) be the right hand side of the\\nlast inequality. Next, we solve again the differential inequality given by Corollary 4.5 for the range\\nt ∈ [ts , 1] with the boundary condition F (y(ts )) ≥ v. The resulting solution is\\n\\x02\\n\\x01\\n\\x03\\nF (y(t)) ≥ e−t (t − ts ) ets · f (OP T ) − (ets − 1) · f (Z ∪ OP T ) + vets\\nPlugging t = 1 and the value of v we get\\n\\x02\\n\\x01\\n\\x03\\nF (y(1)) ≥ e−1 (1 − ts ) ets · f (OP T ) − (ets − 1) · f (Z ∪ OP T ) + vets\\n\\x01\\n1 − ts ts\\ne · f (OP T ) − (ets − 1) · f (Z ∪ OP T )\\n(12)\\n≥\\ne\\n+ ets −1 · {(1 − e−ts ) · [f (OP T ) − f (OP T ∩ Z)] − (1 − e−ts − ts e−ts ) · f (Z ∪ OP T )}\\n= ets −1 · [(2 − ts − e−ts ) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] ,\\nwhere Inequality (12) follows since, by the submodularity and non-negativity of f ,\\nf (OP T \\\\ Z) ≥ f (OP T ) − f (OP T ∩ Z) + f (∅) ≥ f (OP T ) − f (OP T ∩ Z) .\\n6\\n\\nNote that Corollary 4.5 follows from a weaker version of Lemma 4.4 which only guarantees F (y(t) ∨ 1A ) ≥\\n(e\\n− e−t ) · [f (A) − f (A ∪ Z)] + e−t · f (A). We proved the stronger version of the lemma above because\\nit is useful in the formal proof of Theorem 4.1 given in Appendix A.\\n− max{0,t−ts }\\n\\n10\\n\\n\\x0cReferences\\n[1] A. A. Ageev and M. I. Sviridenko. An 0.828 approximation algorithm for the uncapacitated\\nfacility location problem. Discrete Appl. Math., 93:149–156, July 1999.\\n[2] Noga Alon and Joel H. Spencer. The Probabilistic Method. Wiley Interscience, second edition,\\n2000.\\n[3] Per Austrin, Siavosh Benabbas, and Konstantinos Georgiou. Better balance by being biased:\\nA 0.8776-approximation for max bisection. In SODA, pages 277–294, 2013.\\n[4] Francis Bach. Learning with submodular functions: A convex optimization perspective. Foundations and Trends in Machine Learning, 6(2-3):145–373, 2013.\\n[5] Y. Y. Boykov and M. P. Jolly. Interactive graph cuts for optimal boundary & region segmentation of objects in N-D images. In ICCV, volume 1, pages 105–112, 2001.\\n[6] Niv Buchbinder, Moran Feldman, Joseph (Seffi) Naor, and Roy Schwartz. A tight linear time\\n(1/2)-approximation for unconstrained submodular maximization. In FOCS, pages 649–658,\\n2012.\\n[7] Niv Buchbinder, Moran Feldman, Joseph (Seffi) Naor, and Roy Schwartz. Submodular maximization with cardinality constraints. In SODA, pages 1433–1452, 2014.\\n[8] Gruia Călinescu, Chandra Chekuri, Martin Pál, and Jan Vondrák. Maximizing a monotone\\nsubmodular function subject to a matroid constraint. SIAM J. Comput., 40(6):1740–1766,\\n2011.\\n[9] Chandra Chekuri and Alina Ene. Approximation algorithms for submodular multiway partition. In FOCS, pages 807–816, 2011.\\n[10] Chandra Chekuri and Sanjeev Khanna. A polynomial time approximation scheme for the\\nmultiple knapsack problem. SIAM J. Comput., 35(3):713–728, September 2005.\\n[11] Chandra Chekuri, Jan Vondrák, and Rico Zenklusen. Dependent randomized rounding via\\nexchange properties of combinatorial structures. In FOCS, pages 575–584, 2010.\\n[12] Chandra Chekuri, Jan Vondrák, and Rico Zenklusen. Submodular function maximization via\\nthe multilinear relaxation and contention resolution schemes. In STOC, pages 783–792, 2011.\\n[13] Chandra Chekuri, Jan Vondrák, and Rico Zenklusen. Submodular function maximization via\\nthe multilinear relaxation and contention resolution schemes. SIAM J. Comput., 43(6):1831–\\n1879, 2014.\\n[14] Reuven Cohen, Liran Katzir, and Danny Raz. An efficient approximation for the generalized\\nassignment problem. Information Processing Letters, 100(4):162–166, 2006.\\n[15] M. Conforti and G. Cornuèjols. Submodular set functions, matroids and the greedy algorithm.\\ntight worstcase bounds and some generalizations of the radoedmonds theorem. Disc. Appl.\\nMath., 7(3):251–274, 1984.\\n[16] G. Cornuejols, M. L. Fisher, and G. L. Nemhauser. Location of bank accounts to optimize float:\\nan analytic study of exact and approximate algorithms. Management Sciences, 23:789–810,\\n1977.\\n11\\n\\n\\x0c[17] G. Cornuejols, M. L. Fisher, and G. L. Nemhauser. On the uncapacitated location problem.\\nAnnals of Discrete Mathematics, 1:163–177, 1977.\\n[18] Alina Ene and Huy L. Nguyen. Constrained submodular maximization: Beyond 1/e. In FOCS,\\n2016.\\n[19] Uriel Feige. A threshold of ln n for approximating set cover. J. ACM, 45(4):634–652, 1998.\\n[20] Uriel Feige and Michel X. Goemans. Aproximating the value of two prover proof systems, with\\napplications to max 2sat and max dicut. In ISTCS, pages 182–189, 1995.\\n[21] Uriel Feige, Vahab S. Mirrokni, and Jan Vondrák. Maximizing non-monotone submodular\\nfunctions. SIAM Journal on Computing, 40(4):1133–1153, 2011.\\n[22] Uriel Feige and Jan Vondrák. Approximation algorithms for allocation problems: Improving\\nthe factor of 1 − 1/e. In FOCS, pages 667–676, 2006.\\n[23] Moran Feldman. Maximization Problems with Submodular Objective Functions. PhD thesis,\\nTechnion – Israel Institute of Technology, June 2013.\\n[24] Moran Feldman, Joseph Naor, and Roy Schwartz. A unified continuous greedy algorithm for\\nsubmodular maximization. In FOCS, pages 570–579, 2011.\\n[25] Moran Feldman, Joseph (Seffi) Naor, Roy Schwartz, and Justin Ward. Improved approximations for k-exchange systems. In ESA, pages 784–798, 2011.\\n[26] M. L. Fisher, G. L. Nemhauser, and L. A. Wolsey. An analysis of approximations for maximizing submodular set functions – ii. In Polyhedral Combinatorics, volume 8 of Mathematical\\nProgramming Studies, pages 73–87. Springer Berlin Heidelberg, 1978.\\n[27] Lisa Fleischer, Michel X. Goemans, Vahab S. Mirrokni, and Maxim Sviridenko. Tight approximation algorithms for maximum general assignment problems. In SODA, pages 611–620,\\n2006.\\n[28] Alan M. Frieze and Mark Jerrum. Improved approximation algorithms for max k-cut and max\\nbisection. In IPCO, pages 1–13, 1995.\\n[29] Shayan Oveis Gharan and Jan Vondrák. Submodular maximization by simulated annealing.\\nIn SODA, pages 1098–1117, 2011.\\n[30] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM,\\n42(6):1115–1145, 1995.\\n[31] Eran Halperin and Uri Zwick. Combinatorial approximation algorithms for the maximum\\ndirected cut problem. In SODA, pages 1–7, 2001.\\n[32] Jason Hartline, Vahab Mirrokni, and Mukund Sundararajan. Optimal marketing strategies\\nover social networks. In WWW, pages 189–198, 2008.\\n[33] Johan Hȧstad. Some optimal inapproximability results. J. ACM, 48:798–859, July 2001.\\n[34] D. Hausmann and B. Korte. K-greedy algorithms for independence systems. Oper. Res. Ser.\\nA-B, 22(1):219–228, 1978.\\n12\\n\\n\\x0c[35] D. Hausmann, B. Korte, and T. Jenkyns. Worst case analysis of greedy type algorithms for\\nindependence systems. Math. Prog. Study, 12:120–131, 1980.\\n[36] S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: Coupling edges in graph\\ncuts. 2012 IEEE Conference on Computer Vision and Pattern Recognition, 0:1897–1904, 2011.\\n[37] T. Jenkyns. The efficacy of the greedy algorithm. Cong. Num., 17:341–350, 1976.\\n[38] Richard M. Karp. Reducibility among combinatorial problems. In R. E. Miller and J. W.\\nThatcher, editors, Complexity of Computer Computations, pages 85–103. Plenum Press, 1972.\\n[39] David Kempe, Jon Kleinberg, and Éva Tardos. Maximizing the spread of influence through a\\nsocial network. In SIGKDD, pages 137–146, 2003.\\n[40] Subhash Khot, Guy Kindler, Elchanan Mossel, and Ryan O’Donnell. Optimal inapproximability results for max-cut and other 2-variable csps? SIAM J. Comput., 37:319–357, April\\n2007.\\n[41] S. Khuller, A. Moss, and J. Naor. The budgeted maximum coverage problem. Information\\nProcessing Letters, 70(1):39–45, 1999.\\n[42] B. Korte and D. Hausmann. An analysis of the greedy heuristic for independence systems.\\nAnnals of Discrete Math., 2:65–74, 1978.\\n[43] Andreas Krause, AjitSingh, and Carlos Guestrin. Near-optimal sensor placements in gaussian\\nprocesses: Theory, efficient algorithms and empirical studies. J. Mach. Learn. Res., 9:235–284,\\nJanuary 2008.\\n[44] Andreas Krause and Carlos Guestrin. Near-optimal nonmyopic value of information in graphical models. In UAI, page 5, 2005.\\n[45] Andreas Krause, Jure Leskovec, Carlos Guestrin, Jeanne VanBriesen, and Christos Faloutsos.\\nEfficient sensor placement optimization for securing large water distribution networks. Journal\\nof Water Resources Planning and Management, 134(6):516–526, November 2008.\\n[46] Ariel Kulik, Hadas Shachnai, and Tami Tamir. Approximations for monotone and nonmonotone submodular maximization with knapsack constraints. Math. Oper. Res., 38(4):729–739,\\n2013.\\n[47] Jon Lee, Vahab S. Mirrokni, Viswanath Nagarajan, and Maxim Sviridenko. Maximizing nonmonotone submodular functions under matroid or knapsack constraints. SIAM Journal on\\nDiscrete Mathematics, 23(4):2053–2078, 2010.\\n[48] Jon Lee, Maxim Sviridenko, and Jan Vondrák. Submodular maximization over multiple matroids via generalized exchange properties. In APPROX, pages 244–257, 2009.\\n[49] Hui Lin and Jeff Bilmes. Multi-document summarization via budgeted maximization of submodular functions. In North American chapter of the Association for Computational Linguistics/Human Language Technology Conference (NAACL/HLT-2010), Los Angeles, CA, June\\n2010.\\n[50] Hui Lin and Jeff Bilmes. A class of submodular functions for document summarization. In\\nHLT, pages 510–520, 2011.\\n13\\n\\n\\x0c[51] László Lovász. Submodular functions and convexity. In A. Bachem, M. Grötschel, and B. Korte, editors, Mathematical Programming: the State of the Art, pages 235–257. Springer, 1983.\\n[52] L. Lovász M. Grötschel and A. Schrijver. The ellipsoid method and its consequences in combinatorial optimization. Combinatoria, 1(2):169–197, 1981.\\n[53] G. L. Nemhauser and L. A. Wolsey. Best algorithms for approximating the maximum of a\\nsubmodular set function. Mathematics of Operations Research, 3(3):177–188, 1978.\\n[54] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functionsi. Mathematical Programming, 14:265–294, 1978.\\n[55] Maxim Sviridenko. A note on maximizing a submodular set function subject to knapsack\\nconstraint. Operations Research Letters, 32:41–43, 2004.\\n[56] Luca Trevisan, Gregory B. Sorkin, Madhu Sudan, and David P. Williamson. Gadgets, approximation, and linear programming. SIAM J. Comput., 29:2074–2097, April 2000.\\n[57] Jan Vondrák. Symmetry and approximability of submodular maximization problems. SIAM\\nJ. Comput., 42(1):265–304, 2013.\\n\\nA\\n\\nA Formal Proof of Theorem 4.1\\n\\nIn this section we give a formal proof of Theorem 4.1. This proof is based on the same ideas used\\nin the non-formal proof of this theorem in Section 4, but employs also additional known techniques\\nin order to get rid of the issues that make the proof from Section 4 non-formal.\\nThe algorithm we use to prove Theorem 4.1 is given as Algorithm 3. This algorithm is a discrete\\nvariant of Algorithm 2. While reading the algorithm, it is important to observe that the choice of\\nthe values δ̄1 and δ̄2 guarantees that the variable t takes each one of the values ts and 1 at some\\npoint, and thus, the vectors y(ts ) and y(1) are well defined.\\nAlgorithm 3: Aided Measured Continuous Greedy(f, P, Z, ts )\\n1\\n2\\n\\n3\\n4\\n5\\n\\n6\\n7\\n8\\n9\\n10\\n\\n// Initialization\\nLet δ̄1 ← ts · n−4 and δ̄2 ← (1 − ts ) · n−4 .\\nLet t ← 0 and y(t) ← 1∅ .\\n// Growing y(t)\\nwhile t < 1 do\\nforeach u ∈ N do\\nLet wu (t) be an estimate of E[f (u | R(y(t))] obtained by averaging the values of\\nf (u | R(y(t)) for r = ⌈48n6 ln(2n)⌉ independent samples of R(y(t)).\\nP\\nP\\n\\x1a\\narg maxx∈P { u∈N \\\\Z wu (t) · xu (t) − u∈Z xu (t)} if t ∈ [0, ts ) ,\\n\\x08P\\nLet x(t) ←\\nif t ∈ [ts , 1) .\\narg maxx∈P\\nu∈N wu (t) · xu (t)\\nLet δt be δ̄1 when t < ts and δ̄2 when t ≥ ts .\\nLet y(t + δt ) ← y(t) + δt (1N − y(t)) ◦ x(t).\\nUpdate t ← t + δt .\\nreturn y(1).\\n\\n14\\n\\n\\x0cWe begin the analysis of Algorithm 3 by showing that y(t) remains within the cube [0, 1]N\\nthroughout the execution of the algorithm. Without this observation, the algorithm is not welldefined.\\nObservation A.1. For every value of t, y(t) ∈ [0, 1]N .\\nProof. We prove the observation by induction on t. Clearly the observation holds for y(0) = 1∅ .\\nAssume the observation holds for some time t, then, for every u ∈ N ,\\nyu (t + δt ) = yu (t) + δt (1 − yu (t)) · xu (t) ≥ 0 ,\\nwhere the inequality holds since the induction hypothesis implies 1 − yu (t) ∈ [0, 1]. A similar\\nargument also implies\\nyu (t + δt ) = yu (t) + δt (1 − yu (t)) · xu (t) ≤ yu (t) + (1 − yu (t)) = 1 .\\nUsing the last observation it is now possible to prove the following counterpart of Observation 4.2.\\nCorollary A.2. Algorithm 3 always outputs a vector in P .\\nProof. Let T be the set of values t takes\\nP\\nP during the execution of Algorithm 3. We observe that\\nδ\\n=\\n1,\\nwhich\\nimplies\\nthat\\nt∈T \\\\{1} t\\nt∈T \\\\{1} δt · x(t) is a convex combination of the vectors\\n{x(t) : t ∈ T \\\\ {1}}.\\nP As all these vectors belong to P , and P is convex, any convex combination of\\nthem, including t∈T \\\\{1} δt · x(t), must be in P .\\nNext, we rewrite the output of Algorithm 3 as\\nX\\nX\\nδt · x(t) .\\nδt (1N − y(t)) ◦ x(t) ≤\\ny(1) =\\nt∈T \\\\{1}\\n\\nt∈T \\\\{1}\\n\\nBy the above discussion the rightmost hand side of this inequality is a vector in P , which implies\\nthat y(1) ∈ P since P is down-closed.\\nThe next step towards showing that Algorithm 3 proves Theorem 4.1 is analyzing its approximation ratio. We start this analysis by showing that with high probability all the estimations made by\\nthe algorithm are quite accurate. Let A be the event that |wu (t)−E[f (u | R(y(t)))]| ≤ n−2 ·f (OP T )\\nfor every u ∈ N and time t.\\nLemma A.3 (The symmetric version of Theorem A.1.16 in [2]). Let Xi , 1 ≤ i ≤ k, be mutually\\nindependent with all E[Xi ] = 0 and all |Xi | ≤ 1. Set S = X1 + · · · + Xk . Then, Pr[|S| > a] ≤\\n2\\n2e−a /2k .\\nCorollary A.4. Pr[A] ≥ 1 − n−1 .\\nProof. Consider the calculation of wu (t) for a given u ∈ N and time t. This calculation is done by\\naveraging the value of f (u | R(y(t))) for r independent samples of R(y(t)). Let Yi denote the value\\n(u|R(y(t)))]\\n. Then, by definition,\\nof f (u | R(y(t))) obtained for the i-th sample, and let Xi = Yi −E[f\\n2n·f (OP T )\\nwu (t) =\\n\\nPr\\n\\ni=1 Yi\\n\\nr\\n\\n= [2n · f (OP T )] ·\\n\\nPr\\n\\ni=1 Xi\\n\\nr\\n\\n+ E[f (u | R(y(t)))] .\\n\\nSince Yi is distributed like f (u | R(y(t))), the definition of Xi guarantees that E[Xi ] = 0 for\\nevery 1 ≤ i ≤ r. Additionally, |Xi | ≤ 1 for every such i since the absolute values of both Yi and\\n15\\n\\n\\x0cE[f (u | R(y(t)))] are upper bounded by maxS⊆N f (S) ≤ n · f (OP T ) (the last inequality follows\\nfrom our assumption that 1u ∈ P for every element u ∈ N ). Thus, by Lemma A.3,\\n#\\n\" r\\nX\\nr\\n−3\\n2\\nXi > 3 ≤ 2e−[rn /2] /2r\\nPr[|wu (t) − E[f (u | R(y(t)))]| > n−2 · f (OP T )] = Pr\\n2n\\ni=1\\n\\x12 \\x136\\n1\\n1\\n−rn−6 /8\\n−6 ln(2n)\\n= 2e\\n≤ 2e\\n=2·\\n≤ 6 .\\n2n\\n2n\\nObserve that Algorithm 3 calculates wu (t) for every combination of element u ∈ N and time\\nt < 1. Since there are n elements in N and 2n4 times smaller than 1, the union bound implies\\nthat the probability that for at least one such value wu (t) we have |wu (t) − E[f (u | R(y(t)))]| >\\nn−2 · f (OP T ) is upper bounded by\\n\\x01 1\\n1\\n· n · 2n4 =\\n,\\n6\\n2n\\nn\\n\\nwhich completes the proof of the corollary.\\n\\nOur next step is to give a lower bound on the increase in F (y(t)) as a function of t given A. This\\nlower bound is given by Corollary A.7, which follows from the next two lemmata. The statement\\nand proof of the corollary and the next lemma is easier with the following definition. Let OP Tt′\\ndenote the set OP T \\\\ Z when t < ts , and OP T otherwise.\\nP\\nLemma A.5. Given A, for every time t < 1,\\nu∈N (1 − yu (t)) · xu (t) · ∂u F (y(t)) ≥ F (y(t) ∨\\n−1\\n1OP Tt′ ) − F (y(t)) − O(n ) · f (OP T ).\\nProof. Let us calculate the weight of OP Tt′ according to the weight function w(t).\\nX\\nX\\nwu (t) ≥\\n[E[f (u | R(y(t)))] − n−2 · f (OP T )]\\nw(t) · 1OP Tt′ =\\nu∈OP Tt′\\n\\n\\uf8ee\\n\\n≥ E\\uf8f0\\n\\nX\\n\\nu∈OP Tt′\\n\\nu∈OP Tt′\\n\\n\\uf8f9\\n\\nf (R(y(t)) + u) − f (R(y(t)))\\uf8fb − n−1 · f (OP T )\\n\\n\\x03\\n≥ E f (R(y(t)) ∪ OP Tt′ ) − f (R(y(t))) − n−1 · f (OP T )\\n\\x02\\n\\n= F (y(t) ∨ 1OP T ′t ) − F (y(t)) − n−1 · f (OP T ) ,\\n\\nwhere the first inequality follows from the definition of A, and the last follows from the submodularity of f . Recall that x(t) is the vector in P maximizing some objective function (which depends on\\nt). For t < ts , the objective function maximized by x(t) assigns the value w(t)·1OP T \\\\Z = w(t)·1OP Tt′\\nto the vector 1OP Tt′ ∈ P . Similarly, for t ≥ ts , the objective function maximized by x(t) assigns the\\nvalue w(t) · 1OP T = w(t) · 1OP Tt′ to the vector 1OP Tt′ ∈ P . Thus, the definition of x(t) guarantees\\nthat in both cases we have\\nw(t) · x(t) ≥ w(t) · 1OP Tt′ ≥ F (y(t) ∨ 1OP Tt′ ) − F (y(t)) − n−1 · f (OP T ) .\\n\\n16\\n\\n\\x0cHence,\\nX\\n\\n(1 − yu (t)) · xu (t)·∂u F (y(t)) =\\n\\nu∈N\\n\\nX\\n\\nxu (t) · [F (y(t) ∨ 1u ) − F (y(t))]\\n\\nu∈N\\n\\nX\\n\\n=\\n\\nxu (t) · E[f (u | R(y(t)))]\\n\\nu∈N\\n\\nX\\n\\n≥\\n\\nxu (t) · [wu (t) − n−2 · f (OP T )] = x(t) · w(t) − n−1 · f (OP T )\\n\\nu∈N\\n\\n≥ F (y(t) ∨ 1OP Tt′ ) − F (y(t)) − 2n−1 · f (OP T ) ,\\nwhere the first inequality holds by the definition of A and the second equality holds since\\nF (y(t) ∨ 1u ) − F (y(t)) = E[f (R(y(t)) + u)] − E[f (R(y(t)))] = E[f (u | R(y(t)))] .\\nLemma A.6 (A rephrased version of Lemma 2.3.7 in [23]). Consider\\ntwo vectors x, x′ ∈ [0, 1]N\\nP\\nsuch that |xu − x′u | ≤ δ for every u ∈ N . Then, F (x′ ) − F (x) ≥ u∈N (x′u − xu ) · ∂u F (x) − O(n3 δ2 ) ·\\nmaxu∈N f ({u}).\\nCorollary A.7. Given A, for every time t < 1, F (y(t + δt )) − F (y(t)) ≥ δt [F (y(t) ∨ 1OP Tt′ ) −\\nF (y(t))] − O(n−1 δt ) · f (OP T ).\\nProof. Observe that for every u ∈ N , |yu (t + δt ) − yu (t)| = |δt (1 − yu (t))xu (t)| ≤ δt . Hence, by\\nLemma A.6,\\nX\\nF (y(t + δt )) − F (y(t)) ≥\\n[yu (t + δt )) − yu (t)] · ∂u F (y(t)) − O(n3 δt2 ) · max f ({u})\\nu∈N\\n\\nu∈N\\n\\n=\\n\\nX\\n\\nδt (1 − yu (t)) · xu (t) · ∂u F (y(t)) − O(n3 δt2 ) · max f ({u}) .\\nu∈N\\n\\nu∈N\\n\\n(13)\\n\\nConsider the rightmost hand side of the last inequality. By Lemma A.5, the first term on this side\\ncan be bounded by\\nX\\nδt (1 − yu (t)) · xu (t) · ∂u F (y(t)) ≥ δt · [F (y(t) ∨ 1OP Tt′ ) − F (y(t)) − O(n−1 ) · f (OP T )]\\nu∈N\\n\\n= δt · [F (y(t) ∨ 1OP Tt′ ) − F (y(t))] − O(n−1 δt ) · f (OP T ) .\\n\\nOn the other hand, the second term of (13) can be bounded by\\nO(n3 δt2 ) · max f ({u}) = O(n−1 δt ) · f (OP T )\\nu∈N\\n\\nsince δt ≤ n−4 by definition and maxu∈N f ({u}) ≤ f (OP T ) by our assumption that 1u ∈ P for\\nevery u ∈ N .\\nThe lower bound given by the last corollary is in terms of F (y(t) ∨ 1OP Tt′ ). To make this lower\\nbound useful, we need to lower bound the term F (y(t) ∨ 1OP Tt′ ). This is done by the following two\\nlemma which corresponds to Lemma 4.4.\\nLemma A.8. [corresponds to Lemma 4.4] For every time t < 1 and set A ⊆ N it holds that\\n\\x10\\n\\x11\\nF (y(t) ∨ 1A ) ≥ e− max{0,t−ts } − e−t − O(n−4 ) · max {0, f (A) − f (A ∪ Z)}\\n+ (e−t − O(n−4 )) · f (A) .\\n17\\n\\n\\x0cThe proof of this lemma goes along the same lines as the proof of its corresponding lemma in\\nSection 4, except that the bounds on the coordinates of y(t) used by the proof from Section 4 are\\nreplaced with the (slightly weaker) bounds given by the following lemma.\\nLemma A.9. For every time t and element u ∈ N ,\\n(\\n1 − e−t + O(n−4 )\\nyu (t) ≤\\n1 − e− max{0,t−ts } + O(n−4 )\\n\\nif u 6∈ Z ,\\nif u ∈ Z .\\n\\nProof. Let ε = n−4 , and observe that δt ≤ ε for every time t. Our first objective is to prove by\\ninduction on t that, if yu (τ ) = 0 for some time τ ∈ [0, 1], then yu (t) ≤ 1 − (1 − ε)(t−τ )/ε for every\\ntime t ∈ [τ, 1]. For t = τ the claim holds because yu (τ ) = 0 = 1 − (1 − ε)(τ −τ )/ε . Next, assume the\\nclaim holds for some t, and let us prove it for t + δt .\\nyu (t + δt ) = yu (t) + δt (1 − yu (t)) · xu (t) ≤ yu (t) + δt (1 − yu (t)) = yu (t)(1 − δt ) + δt\\n≤ (1 − (1 − ε)(t−τ )/ε )(1 − δt ) + δt = 1 − (1 − δt )(1 − ε)(t−τ )/ε\\n≤ 1 − (1 − ε)δt /ε (1 − ε)(t−τ )/ε = 1 − (1 − ε)(t+δt −τ )/ε ,\\nwhere the last inequality holds since (1 − x)1/x is a decreasing function for x ∈ (0, 1].\\nWe complete the proof for the case u 6∈ Z by choosing τ = 0 (clearly yu (0) = 0) and observing\\nthat, for every time t,\\n1 − (1 − ε)t/ε ≤ 1 − [e−1 (1 − ε)]t = 1 − e−t (1 − ε)t ≤ 1 − e−t (1 − ε) = 1 − e−t + O(ε) .\\nIt remains to prove the lemma for the case u ∈ Z. Note that at every time t ∈ [0, ts ) Algorithm 3\\nchooses as x(t) a vector maximizing a linear function in P which assigns a negative weight to\\nelements of Z. Since P is down-closed this maximum must have xu (t) = 0 for an element u ∈ Z.\\nThis means that yu (t) = 0 for t ∈ [0, ts ]. In addition to proving the lemma for this time range, the\\nlast inequality also allows us to choose τ = ts , which gives, for t ∈ [ts , 1],\\nyu (t) ≥ 1 − (1 − ε)(t−ts )/ε ≥ 1 − ets −t + O(ε) .\\nCombining Corollary A.7 with Lemma A.8 gives us the following corollary.\\nCorollary A.10. Given A, for every time t ∈ [0, ts ),\\nF (y(t + δt )) − F (y(t)) ≥ δt [f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T ) − F (y(t)))]\\n− O(n−1 δt ) · f (OP T )\\nand, for every time t ∈ [ts , 1),\\nF (y(t + δt )) − F (y(t)) ≥ δt [e−t · f (OP T ) + (ets −t − e−t ) · max{f (OP T ) − f (Z ∪ OP T ), 0}\\n− F (y(t))] − O(n−1 δt ) · f (OP T ) .\\nProof. For every time t ∈ [0, ts ), Corollary A.7 and Lemma A.8 imply together\\nF (y(t + δt )) − F (y(t)) ≥ δt [(1 − e−t − O(n−4 )) · max {0, f (OP T \\\\ Z) − f (OP T ∪ Z)}\\n+ (e−t − O(n−4 )) · f (OP T \\\\ Z)] − O(n−1 δt ) · f (OP T )\\n≥ δt [(1 − O(n−4 )) · f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T )\\n− F (y(t)))] − O(n−1 δt ) · f (OP T ) .\\n18\\n\\n\\x0cWe observe that this inequality is identical to the inequality promised for this time range by the\\ncorollary, except that it has an extra term of −δt · O(n−4 ) · f (OP T \\\\ Z) on its right hand side. Since\\nf (OP T \\\\ Z) is upper bounded by f (OP T ), due to the down-closeness of P , the absolute value of\\nthis extra term is at most\\nδt · O(n−4 ) · f (OP T ) = O(n−1 δt ) · f (OP T ) ,\\nwhich completes the proof for the time range t ∈ [0, ts ).\\nConsider now the time range t ∈ [ts , 1). For this time range Corollary A.7 and Lemma A.8\\nimply together\\nF (y(t + δt )) − F (y(t)) ≥ δt [(ets −t − e−t − O(n−4 )) · max {0, f (OP T ) − f (OP T ∪ Z)}\\n+ (e−t − O(n−4 )) · f (OP T )] − O(n−1 δt ) · f (OP T ) .\\nWe observe again that this inequality is identical to the inequality promised for this time range\\nby the corollary, except that it has extra terms of −δt · O(n−4 ) · f (OP T ) and −δt · O(n−4 ) ·\\nmax{0, f (OP T ) − f (OP T ∪ Z)} on its right hand side. The corollary now follows since the absolute\\nvalue of both these terms is upper bounded by O(n−1 δt ) · f (OP T ).\\nCorollary A.10 bounds the increase in F (y(t)) in terms of F (y(t)) itself. Thus, it gives a\\nrecursive formula which can be used to lower bound F (y(t)). Our remaining task is to solve this\\nformula and get a closed-form lower bound on F (y(t)). Let g(t) be defined as follows. g(0) = 0 and\\nfor every time t < 1,\\ng(t+δt )\\n(\\ng(t) + δt [f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T ) − g(t)]\\n=\\ng(t) + δt [e−t · f (OP T ) + (ets −t − e−t ) · max{f (OP T ) − f (Z ∪ OP T ), 0} − g(t)]\\n\\nif t < ts ,\\nif t ≥ ts .\\n\\nThe next lemma shows that a lower bound on g(t) yields a lower bound on F (y(t)).\\nLemma A.11. Given A, for every time t, g(t) ≤ F (y(t)) + O(n−1 ) · t · f (OP T ).\\nProof. Let c be the larger constant among the constants hiding behind the big O notations in\\nCorollary A.10. We prove by induction on t that g(t) ≤ F (y(t)) + (ct/n) · f (OP T ). For t = 0, this\\nclearly holds since g(0) = 0 ≤ F (y(0)). Assume now that the claim holds for some t, and let us\\nprove it for t + δt . There are two cases to consider. If t < ts , then the induction hypothesis and\\nCorollary A.10 imply, for a large enough n,\\ng(t + δt ) = g(t) + δt [f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T ) − g(t)]\\n= (1 − δt )g(t) + δt [f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T )]\\n≤ (1 − δt )[F (y(t)) + (ct/n) · f (OP T )] + δt [f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T )]\\n= F (y(t)) + δt [f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T ) − F (y(t))]\\n+ (ct/n) · (1 − δt ) · f (OP T )\\n≤ F (y(t + δt )) + (cδt /n) · f (OP T ) + (ct/n) · (1 − δt ) · f (OP T )\\n≤ F (y(t + δt )) + [c(t + δt )/n] · f (OP T ) .\\nSimilarly, if t ≥ ts , then we get\\ng(t + δt ) = g(t) + δt [e−t · f (OP T ) + (ets −t − e−t ) · max{f (OP T ) − f (Z ∪ OP T ), 0} − g(t)]\\n19\\n\\n\\x0c= (1 − δt )g(t) + δt [e−t · f (OP T ) + (ets −t − e−t ) · max{f (OP T ) − f (Z ∪ OP T ), 0}]\\n≤ (1 − δt )[F (y(t)) + (ct/n) · f (OP T )]\\n+ δt [e−t · f (OP T ) + (ets −t − e−t ) · max{f (OP T ) − f (Z ∪ OP T ), 0}]\\n= F (y(t)) + δt [e−t · f (OP T ) + (ets −t − e−t ) · max{f (OP T ) − f (Z ∪ OP T ), 0} − F (y(t))]\\n+ (ct/n) · (1 − δt ) · f (OP T )\\n≤ F (y(t + δt )) + (cδt /n) · f (OP T ) + (ct/n) · (1 − δt ) · f (OP T )\\n≤ F (y(t + δt )) + [c(t + δt )/n] · f (OP T ) .\\nIt remains to find a closed-form expression that lower bounds g(t) (and thus, also F (y(t))). Let\\nh1 (t) : [0, ts ] → R and h2 (t) : [ts , 1] → R be defined as follows.\\nh1 (t) = (1 − e−t ) · f (OP T \\\\ Z) − (1 − e−t − te−t ) · f (Z ∪ OP T ) ,\\nand\\nh2 (t) = e−t · {(t − ts ) · [f (OP T ) + (ets − 1) · max{f (OP T ) − f (OP T ∪ Z), 0}] + ets · h1 (ts )} .\\nLemma A.12. For every time t ≤ ts , h1 (t) ≤ g(t).\\nProof. The proof is by induction on t. For t = 0, g(0) = 0 = (1 − e0 ) · f (OP T \\\\ Z) − (1 − e0 − 0 ·\\ne0 ) · f (Z ∪ OP T ) = h1 (0). Assume now that the lemma holds for some t < ts , and let us prove it\\nholds also for t + δt . By the induction hypothesis,\\nZ t+δt\\nh′ (τ )dτ\\nh1 (t + δt ) = h1 (t) +\\nt\\nZ t+δt\\n{e−τ · f (OP T \\\\ Z) − τ e−τ · f (Z ∪ OP T )}dτ\\n= h1 (t) +\\nt\\n\\n≤ h1 (t) + δt · {e−t · f (OP T \\\\ Z) − te−t · f (Z ∪ OP T )}dτ\\n\\n= (1 − δt )h1 (t) + δt · {f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T )}\\n≤ (1 − δt )g(t) + δt · {f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T )} = g(t + δt ) ,\\nwhere the first inequality holds since e−τ is a decreasing function of τ and τ e−τ is an increasing\\nfunction of τ in the range τ ∈ [0, 1].\\nLemma A.13. For every time ts ≤ t ≤ 1, h2 (t) ≤ g(t).\\nProof. The proof is by induction on t. For t = ts , by Lemma A.12, h2 (ts ) = h1 (ts ) ≤ g(ts ). Assume\\nnow that the lemma holds for some ts ≤ t < 1, and let us prove it holds also for t + δt .\\nTo avoid repeating complex expressions, let us denote A = f (OP T ) + (ets − 1) · max{f (OP T ) −\\nf (Z ∪ OP T ), 0}. Notice that A is independent of t. Moreover, using this notation we can rewrite\\nh2 (t) as h2 (t) = e−t · {(t − ts ) · A + ets · h1 (ts )}. Thus, for every τ ∈ (ts , 1),\\nh′2 (τ ) = −e−τ · {(τ − ts ) · A + ets · h1 (ts )} + e−τ · A = e−τ · {(1 − τ + ts ) · A − ets · h1 (ts )} .\\nThe definition of A and the non-negativity of f imply immediately that A ≥ 0. We would like\\nto prove also that ts · A − ets · h1 (ts ) ≥ 0. There are two cases to consider. First, if f (OP T ) ≥\\nf (Z ∪ OP T ), then\\nts · A − ets · h1 (ts ) = ts · f (OP T ) + ts (ets − 1) · max{f (OP T ) − f (Z ∪ OP T ), 0}\\n20\\n\\n\\x0c− (ets − 1) · f (OP T \\\\ Z) + (ets − 1 − ts ) · f (Z ∪ OP T )\\n≥ ts ets · f (OP T ) − ts (ets − 1) · f (Z ∪ OP T )\\n− (ets − 1) · f (OP T ) + (ets − 1 − ts ) · f (Z ∪ OP T )\\n= (ts ets − ets + 1) · [f (OP T ) − f (Z ∪ OP T )] ≥ 0 .\\nwhere the inequality uses the fact that f (OP T ) ≥ f (OP T \\\\ Z) because of the down-closure of P .\\nOn the other hand, if f (OP T ) < f (Z ∪ OP T ), then\\nts · A − ets · h1 (ts ) = ts · f (OP T ) − (ets − 1) · f (OP T \\\\ Z) + (ets − 1 − ts ) · f (Z ∪ OP T )\\n≥ ts · f (OP T ) − (ets − 1) · f (OP T ) + (ets − 1 − ts ) · f (OP T ) = 0 .\\nUsing the above observations and the induction hypothesis, we can now get\\nh2 (t + δt ) = h2 (t) +\\n\\nZ\\n\\nt+δt\\n\\nh′ (τ )dτ = h2 (t) +\\n\\nZ\\n\\nt+δt\\n\\ne−τ · {(1 − τ + ts ) · A − ets · h1 (ts )}dτ\\n\\nt\\n\\nt\\n\\n≤ h2 (t) + δt · e−t · {(1 − t + ts ) · A − ets · h1 (ts )} = (1 − δt )h2 (t) + δt · e−t · A\\n≤ (1 − δt )g(t) + δt · e−t · A = g(t + δt ) .\\nThe last two lemmata give us the promised closed-form lower bound on g(t), which can be used\\nto lower bound the approximation ratio of Algorithm 3.\\nCorollary A.14. E[F (y(1))] ≥ ets −1 · [(2 − ts − e−ts − O(n−1 )) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T ) −\\n(2 − ts − 2e−ts ) · f (Z ∪ OP T )].\\nProof. By Lemma A.11, given A,\\nF (y(1)) ≥ g(1) − O(n−1 ) · f (OP T ) .\\nBy Lemma A.13,\\ng(1) ≥ h2 (1)\\n= e−1 · {(1 − ts ) · [f (OP T ) + (ets − 1) · max{f (OP T ) − f (Z ∪ OP T ), 0}]\\n+ (ets − 1) · f (OP T \\\\ Z) − (ets − 1 − ts ) · f (Z ∪ OP T )}\\n≥ e−1 · {(1 − ts ) · [ets · f (OP T ) − (ets − 1) · f (Z ∪ OP T )]\\n+ (ets − 1) · [f (OP T ) − f (Z ∩ OP T )] − (ets − 1 − ts ) · f (Z ∪ OP T )}\\n= ets −1 · [(2 − ts − e−ts ) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] ,\\nwhere the second inequality holds since the submodularity and non-negativity of f imply\\nf (OP T \\\\ Z) ≥ f (OP T ) + f (∅) − f (Z ∩ OP T ) ≥ f (OP T ) − f (Z ∩ OP T ) .\\nCombining the above observations we get that, given A,\\nF (y(1)) ≥ ets −1 · [(2 − ts − e−ts − O(n−1 )) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] .\\n\\n21\\n\\n\\x0cSince F (y(1)) is always non-negative, this implies, by the law of total expectation,\\nE[F (y(1))] ≥ Pr[A] · {ets −1 · [(2 − ts − e−ts − O(n−1 )) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )]}\\n≥ {ets −1 · [(2 − ts − e−ts − O(n−1 )) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )]}\\n1\\n− · ets −1 · (2 − ts − e−ts − O(n−1 )) · f (OP T )\\nn\\n= ets −1 · [(2 − ts − e−ts − O(n−1 )) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] ,\\nwhere the second inequality holds since Pr[A] ≥ 1 − n−1 by Corollary A.4.\\nTheorem 4.1 now follows immediately by combining Corollaries A.2 and A.14.\\n\\n22\\n\\n\\x0c',\n",
       " 'label': 8}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize a sample.\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16fdb14",
   "metadata": {},
   "source": [
    "## Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038f40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"math.AC\",\n",
    "    1: \"cs.CV\",\n",
    "    2: \"cs.AI\",\n",
    "    3: \"cs.SY\",\n",
    "    4: \"math.GR\",\n",
    "    5: \"cs.CE\",\n",
    "    6: \"cs.PL\",\n",
    "    7: \"cs.IT\",\n",
    "    8: \"cs.DS\",\n",
    "    9: \"cs.NE\",\n",
    "    10: \"math.ST\"\n",
    "}\n",
    "label2id = {\n",
    "    \"math.AC\": 0,\n",
    "    \"cs.CV\": 1,\n",
    "    \"cs.AI\": 2,\n",
    "    \"cs.SY\": 3,\n",
    "    \"math.GR\": 4,\n",
    "    \"cs.CE\": 5,\n",
    "    \"cs.PL\": 6,\n",
    "    \"cs.IT\": 7,\n",
    "    \"cs.DS\": 8,\n",
    "    \"cs.NE\": 9,\n",
    "    \"math.ST\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768947a9",
   "metadata": {},
   "source": [
    "## Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb40ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a857a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for preprocessing.\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1edc05de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df823f120b347abb35d6cea1dbb022e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/28388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_proc=NUM_PROCS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fb91399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8dea3b939f4236a6d0465791795b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_valid = valid_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_proc=NUM_PROCS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08643985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcdd44cfe9741acbd518cc6c51ba172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_test = test_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_proc=NUM_PROCS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e951263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data collator.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fe3c23",
   "metadata": {},
   "source": [
    "## Sample Tokenization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b7e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sample = preprocess_function(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ab1eb7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 27570, 4942, 5302, 8566, 8017, 20446, 3989, 3081, 1037, 2512, 1011, 19490, 6028, 12098, 9048, 2615, 1024, 28769, 1012, 6021, 17788, 2509, 2615, 2487, 1031, 20116, 1012, 16233, 1033, 2184, 13292, 2355, 9152, 2615, 20934, 2818, 8428, 4063, 30125, 17866, 26908, 1526, 2281, 2340, 1010, 2355, 10061, 1996, 2817, 1997, 22863, 23207, 4818, 20600, 3471, 2007, 1037, 4942, 5302, 8566, 8017, 7863, 2038, 6296, 2172, 3086, 1999, 3522, 2086, 1012, 2107, 3471, 2024, 2590, 1999, 2119, 3399, 1998, 3218, 2138, 2037, 7863, 4972, 2024, 2200, 2236, 1012, 11381, 2582, 8377, 2005, 2116, 4942, 5302, 8566, 8017, 20446, 3989, 3471, 26077, 2015, 2091, 2000, 4531, 2488, 13792, 2005, 23569, 27605, 6774, 1037, 23370, 1997, 2068, 2124, 2004, 1996, 4800, 4179, 2906, 5331, 1012, 1999, 2023, 2147, 2057, 2556, 2019, 9896, 2005, 23569, 27605, 6774, 1996, 4800, 4179, 2906, 23370, 3005, 11302, 24840, 2058, 1996, 11302, 1997, 1996, 2190, 3025, 9896, 1006, 2029, 2001, 2445, 2011, 4372, 2063, 1998, 16577, 1006, 2355, 1007, 1007, 1012, 9308, 1010, 2256, 9896, 2003, 2241, 2006, 1037, 2047, 6028, 2029, 2003, 1010, 15835, 1010, 16325, 1998, 2062, 3019, 2005, 1996, 3291, 2012, 2192, 1012, 1999, 1037, 12264, 18223, 1010, 3025, 13792, 2005, 2023, 3291, 11160, 2006, 14991, 5144, 2029, 2024, 3019, 2069, 1999, 1996, 6438, 1997, 1037, 27142, 1012, 2256, 6028, 26777, 1996, 2342, 2000, 7001, 2000, 2107, 5144, 1010, 1998, 2947, 1010, 3849, 2000, 2022, 1037, 2488, 4906, 2005, 27570, 3471, 1012, 1598, 2533, 1997, 6747, 1998, 3136, 2470, 1010, 2082, 1997, 8045, 4163, 1010, 10093, 12724, 2118, 1010, 3956, 1012, 10373, 1024, 9152, 2615, 1012, 20934, 2818, 8428, 4063, 1030, 20917, 4014, 1012, 4012, 1012, 1526, 18280, 1012, 1997, 5597, 1998, 3274, 2671, 1010, 1996, 2330, 2118, 1997, 3956, 1012, 10373, 1024, 17866, 7959, 1030, 2330, 2226, 1012, 9353, 1012, 6335, 1012, 1015, 4955, 1996, 2817, 1997, 22863, 23207, 4818, 20600, 3471, 2007, 1037, 4942, 5302, 8566, 8017, 7863, 2038, 6296, 2172, 3086, 1999, 3522, 2086, 1012, 2107, 3471, 2024, 2590, 1999, 2119, 3399, 1998, 3218, 2138, 2037, 7863, 4972, 2024, 2200, 2236, 1517, 4942, 5302, 8566, 8017, 4972, 2236, 4697, 1010, 2005, 2742, 1010, 7659, 4972, 1997, 19287, 1998, 2856, 19287, 1010, 1996, 8203, 2592, 3853, 1010, 13523, 22943, 18215, 4635, 4972, 1998, 8833, 1011, 28283, 22311, 7666, 1012, 2062, 4919, 1010, 2013, 1037, 9373, 7339, 1010, 2116, 2092, 1011, 2124, 3471, 1999, 22863, 23207, 4818, 20600, 2024, 1999, 2755, 4942, 5302, 8566, 8017, 20446, 3989, 3471, 1010, 2164, 1024, 4098, 1011, 3013, 1031, 2382, 1010, 3943, 1010, 4229, 1010, 2871, 1010, 5179, 1033, 1010, 4098, 1011, 4487, 12690, 1031, 2322, 1010, 2382, 1010, 2861, 1033, 1010, 18960, 8775, 1031, 2184, 1010, 2403, 1010, 2570, 1010, 2676, 1033, 1010, 4098, 1011, 1047, 1011, 6325, 1031, 2539, 1010, 4601, 1033, 1010, 4098, 1011, 20377, 18491, 1031, 1017, 1010, 2654, 1033, 1998, 4322, 3295, 1031, 1015, 1010, 2385, 1010, 2459, 1033, 1012, 2013, 1037, 6742, 7339, 1010, 4942, 5302, 8566, 8017, 20446, 3989, 3471, 2031, 2179, 3594, 1999, 2591, 6125, 1031, 3590, 1010, 4464, 1033, 1010, 4432, 1031, 1019, 1010, 4029, 1033, 1010, 3698, 4083, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Length of tokenized IDs: 512\n",
      "Length of attention mask: 512\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sample)\n",
    "print(f\"Length of tokenized IDs: {len(tokenized_sample.input_ids)}\")\n",
    "print(f\"Length of attention mask: {len(tokenized_sample.attention_mask)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb19955",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03b567a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e4dbae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d067d2f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b3bdbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b165b477e643599826a530e84ed2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL, \n",
    "    num_labels=11,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f19ea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109,490,699 total parameters.\n",
      "109,490,699 training parameters.\n"
     ]
    }
   ],
   "source": [
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743daa6",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60a81b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3,\n",
    "    report_to='tensorboard',\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e91c384",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "530e59c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f821460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4440' max='4440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4440/4440 30:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.821900</td>\n",
       "      <td>0.498572</td>\n",
       "      <td>0.843600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.394700</td>\n",
       "      <td>0.407233</td>\n",
       "      <td>0.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.283700</td>\n",
       "      <td>0.411557</td>\n",
       "      <td>0.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.211700</td>\n",
       "      <td>0.436728</td>\n",
       "      <td>0.870400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.467710</td>\n",
       "      <td>0.863600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3de01",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6235efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.38107171654701233,\n",
       " 'eval_accuracy': 0.8796,\n",
       " 'eval_runtime': 14.9454,\n",
       " 'eval_samples_per_second': 167.276,\n",
       " 'eval_steps_per_second': 2.676,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34c628-fe1e-4315-9f8e-a8b09a2bae7c",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e3857da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220\n"
     ]
    }
   ],
   "source": [
    "print(history.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77977672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(f\"{OUT_DIR}/checkpoint-{history.global_step}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(f\"arxiv_bert/checkpoint-4440\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aabb76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e9dc18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For finitely generated modules M and N over a commutative Noetherian local ring R, we give various sufficient criteria for detecting freeness of M or N via vanishing of some Ext modules ExtiR(M,N) and finiteness of certain homological dimension of HomR(M,N). Some of our results provide partial progress towards answering a question of Ghosh-Takahashi and also generalize their main results in many ways, for instance, by reducing the number of vanishing. As some applications, we provide affirmative answers to two questions raised by Tony Se on n-semidualizing modules. In particular, we establish that for normal domains which satisfy Serre's condition (S3) and are locally Gorenstein in co-dimension two, the class of 1-semidualizing modules form a subgroup of the divisor class group. These two groups coincide when, in addition, the ring is locally regular in co-dimension two.\n",
      "PRED:  [{'label': 'math.AC', 'score': 0.9966996312141418}]\n",
      "GT:  math.ac\n",
      "\n",
      "\n",
      "In this paper we produce the first known formula for cohomologies of the derived tensor products of structure sheaves of subschemes in the case where the intersection of the subschemes is not a local complete intersection. The case covered here is where the intersection instead consists of two local complete intersection components, one of codimension 1 and the other of arbitrary codimension.\n",
      "PRED:  [{'label': 'math.AC', 'score': 0.998441755771637}]\n",
      "GT:  math.ac\n",
      "\n",
      "\n",
      "We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL.\n",
      "PRED:  [{'label': 'cs.CV', 'score': 0.7409049868583679}]\n",
      "GT:  cs.cv\n",
      "\n",
      "\n",
      "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.\n",
      "PRED:  [{'label': 'cs.NE', 'score': 0.6367722153663635}]\n",
      "GT:  cs.cv\n",
      "\n",
      "\n",
      "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\n",
      "This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.\n",
      "PRED:  [{'label': 'cs.CV', 'score': 0.972852885723114}]\n",
      "GT:  cs.cv\n",
      "\n",
      "\n",
      "This paper studies Flag sequences for lowcomplexity delay-Doppler estimation by exploiting their distinctive peak-curtain ambiguity functions (AFs). Unlike the existing Flag sequence designs that are limited to prime lengths and periodic auto-AFs, we aim to design Flag sequence sets of arbitrary lengths and with low (nontrivial) periodic/aperiodic auto- and cross-AFs. Since every Flag sequence consists of a Curtain sequence and a Peak sequence, we first investigate the algebraic design of zone-based Curtain sequence sets of arbitrary lengths. Our proposed design gives rise to novel Curtain sequence sets with ideal curtain auto-AFs and low/zero cross-AFs within the delay-Doppler zone of interest. Leveraging these Curtain sequence sets, two optimization problems are formulated to minimize the summed customized weighted integrated sidelobe level (SCWISL) of the Flag sequence set. Accelerated Parallel Partially Majorization-Minimization Algorithms are proposed to jointly optimize the transmit Flag sequences and matched/mismatched reference sequences stored in the receiver. Simulations demonstrate that our proposed Flag sequences lead to improved SCWISL and customized peak-to-max-sidelobe ratio compared with the existing Flag sequences. Additionally, our Flag sequences under Flag method exhibit Mean Squared Errors that approach the Cramer-Rao Lower Bound and the Sampling Bound at high signal-to-noise power ratios.\n",
      "PRED:  [{'label': 'cs.IT', 'score': 0.6986567974090576}]\n",
      "GT:  cs.it\n",
      "\n",
      "\n",
      "The paper presents a spectral representation for general type two-sided discrete time signals from ℓ∞, i.e for all bounded discrete time signals, including signals that do not vanish at ±∞. This representation allows to extend on the general type signals from ℓ∞ the notions of transfer functions, spectrum gaps, and filters, and to obtain some frequency conditions of predictability and data recoverability.\n",
      "PRED:  [{'label': 'cs.IT', 'score': 0.9719427824020386}]\n",
      "GT:  cs.it\n",
      "\n",
      "\n",
      "Although there is extensive literature on the application of artificial neural networks (NNs) in quality control (QC), to monitor the conformity of a process to quality specifications, at least five QC measurements are required, increasing the related cost. To explore the application of neural networks to samples of QC measurements of very small size, four one-dimensional (1-D) convolutional neural networks (CNNs) were designed, trained, and tested with datasets of n-tuples of simulated standardized normally distributed QC measurements, for 1≤n≤4. The designed neural networks were compared to statistical QC functions with equal probabilities for false rejection, applied to samples of the same size. When the n-tuples included at least two QC measurements distributed as N(μ,σ2), where 0.2<|μ|≤6.0, and 1.0<σ≤7.0, the designed neural networks outperformed the respective statistical QC functions. Therefore, 1-D CNNs applied to samples of 2-4 quality control measurements can be used to increase the probability of detection of the nonconformity of a process to the quality specifications, with lower cost.\n",
      "PRED:  [{'label': 'cs.NE', 'score': 0.9920564293861389}]\n",
      "GT:  cs.ne\n",
      "\n",
      "\n",
      "We investigate the boundary between chemotaxis driven by spatial estimation of gradients and chemotaxis driven by temporal estimation. While it is well known that spatial chemotaxis becomes disadvantageous for small organisms at high noise levels, it is unclear whether there is a discontinuous switch of optimal strategies or a continuous transition exists. Here, we employ deep reinforcement learning to study the possible integration of spatial and temporal information in an a priori unconstrained manner. We parameterize such a combined chemotactic policy by a recurrent neural network and evaluate it using a minimal theoretical model of a chemotactic cell. By comparing with constrained variants of the policy, we show that it converges to purely temporal and spatial strategies at small and large cell sizes, respectively. We find that the transition between the regimes is continuous, with the combined strategy outperforming in the transition region both the constrained variants as well as models that explicitly integrate spatial and temporal information. Finally, by utilizing the attribution method of integrated gradients, we show that the policy relies on a non-trivial combination of spatially and temporally derived gradient information in a ratio that varies dynamically during the chemotactic trajectories.\n",
      "PRED:  [{'label': 'cs.NE', 'score': 0.9100736975669861}]\n",
      "GT:  cs.ne\n",
      "\n",
      "\n",
      "Recently, various Artificial Intelligence (AI) based optimization metaheuristics are proposed and applied for a variety of problems. Cohort Intelligence (CI) algorithm is a socio inspired optimization technique which is successfully applied for solving several unconstrained & constrained real-world problems from the domains such as design, manufacturing, supply chain, healthcare, etc. Generally, real-world problems are constrained in nature. Even though most of the Evolutionary Algorithms (EAs) can efficiently solve unconstrained problems, their performance degenerates when the constraints are involved. In this paper, two novel constraint handling approaches based on modulus and hyperbolic tangent probability distributions are proposed. Constrained CI algorithm with constraint handling approaches based on triangular, modulus and hyperbolic tangent is presented and applied for optimizing advanced manufacturing processes such as Water Jet Machining (WJM), Abrasive Jet Machining (AJM), Ultrasonic Machining (USM) and Grinding process. The solutions obtained using proposed CI algorithm are compared with contemporary algorithms such as Genetic Algorithm, Simulated Annealing, Teaching Learning Based Optimization, etc. The proposed approaches achieved 2%-127% maximization of material removal rate satisfying hard constraints. As compared to the GA, CI with Hyperbolic tangent probability distribution achieved 15%, 2%, 2%, 127%, and 4% improvement in MRR for AJMB, AJMD, WJM, USM, and Grinding processes, respectively contributing to the productivity improvement. The contributions in this paper have opened several avenues for further applicability of the proposed constraint handling approaches for solving complex constrained problems.\n",
      "PRED:  [{'label': 'cs.NE', 'score': 0.7764154076576233}]\n",
      "GT:  cs.ne\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob('inference_data/*')\n",
    "for file_name in all_files:\n",
    "    file = open(file_name) \n",
    "    content = file.read()\n",
    "    print(content)\n",
    "    result = classify(content)\n",
    "    print('PRED: ', result)\n",
    "    print('GT: ', file_name.split('_')[-1].split('.txt')[0])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc205b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69813bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
